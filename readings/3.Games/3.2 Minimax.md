## 3.2 Minimax

我们将要讨论的第一个零和博弈算法是 minimax。该算法基于一个核心假设：我们面对的对手是最优行动的，并且总会执行对我们最不利的动作。为了介绍这个算法，我们必须首先形式化终止效用和状态价值的概念。一个状态的价值，是控制该状态的智能体所能获得的最优得分。为了理解这一点，请观察下面这个极其简单的吃豆人游戏棋盘：

![](../../image/3.Games/easy-pacman.png)

Pacman Game

假设吃豆人初始拥有 10 分，并且在吃到豆子之前每移动一步都会损失 1 分；一旦吃到豆子，游戏到达一个终止状态并结束。我们可以像普通搜索问题中的搜索树一样，为这个棋盘构建一棵博弈树，其中一个状态的子节点是它的后继状态，如下所示：

![](../../image/3.Games/easy-pacman-tree.png)

Pacman Game Tree

从这棵树可以明显看出，如果吃豆人直接走向豆子，他最终会以 8 分结束游戏；而如果他在任何时刻绕路，他最终得到的分数都会更低。现在我们已经生成了一棵包含多个终止状态和中间状态的博弈树，我们可以形式化地定义这些状态的价值含义。

一个状态的价值被定义为：智能体从该状态出发所能获得的最佳可能结果（效用）。我们稍后会更具体地形式化效用的概念，但目前只需将智能体的效用理解为它获得的得分或分数即可。终止状态的价值称为终止效用，它总是某个确定已知的数值，并且是博弈本身的固有属性。在我们的吃豆人示例中，最右侧终止状态的价值就是 8，即吃豆人直接走向豆子所得到的分数。此外，在这个示例中，非终止状态的价值被定义为其所有子节点价值中的最大值。定义  
\( V(s) \) 为表示状态 \( s \) 价值的函数，我们可以总结如下：

\[
\forall \text{非终止状态}, \quad
V(s) = \max_{s' \in successors(s)} V(s')
\]

\[
\forall \text{终止状态}, \quad
V(s) = known
\]

这建立了一个非常简单的递归规则。根据这个规则，根节点的右子节点的价值为 8，而根节点的左子节点的价值为 6，因为这分别是智能体从起始状态向右或向左移动时所能获得的最大可能得分。因此，通过运行这样的计算，智能体可以确定向右移动是最优的，因为右子节点的价值大于左子节点的价值。

现在，让我们引入一个新的游戏棋盘，其中存在一个对抗性的幽灵，它试图阻止吃豆人吃到豆子。

![](../../image/3.Games/pacman-with-ghost.png)

Pacman with Ghost

游戏规则规定两个智能体轮流行动，因此博弈树中两个智能体会在不同层级交替“控制”节点。一个智能体控制某个节点，意味着该节点对应的状态轮到该智能体行动，因此它有机会决定采取哪种行动并相应改变游戏状态。下面是由上述双智能体棋盘产生的博弈树：

![](../../image/3.Games/pacman-with-ghost-full-game-tree.png)

Pacman Game Tree

蓝色节点表示由吃豆人控制的节点，他可以决定采取什么行动；红色节点表示由幽灵控制的节点。注意，所有幽灵控制节点的子节点，都是幽灵从父节点状态向左或向右移动之后的状态；对于吃豆人控制节点亦是如此。为简单起见，我们将这棵博弈树截断为深度为 2 的树，并为终止状态赋予如下伪造的数值：

![](../../image/3.Games/small-game-tree.png)

Small Game Tree

显然，引入幽灵控制节点改变了吃豆人认为最优的行动，而新的最优行动由 minimax 算法决定。minimax 算法并不是在树的每一层都对所有子节点取最大值，而是：在吃豆人控制的节点上取子节点中的最大值，在幽灵控制的节点上取子节点中的最小值。因此，上述两个幽灵节点的价值分别为  

\[
\min(-8, -5) = -8
\]

和  

\[
\min(-10, +8) = -10
\]

相应地，由吃豆人控制的根节点的价值为  

\[
\max(-8, -10) = -8
\]

由于吃豆人希望最大化自己的得分，他会选择向左移动并接受 -8 的得分，而不是尝试走向豆子而得到 -10。这是“通过计算产生行为”的一个典型例子——虽然吃豆人希望得到 +8（如果他最终到达最右侧子节点状态），但通过 minimax，他“知道”一个最优行动的幽灵不会允许他得到这个结果。为了最优行动，吃豆人被迫规避风险，反直觉地远离豆子，以最小化失败的程度。我们可以总结 minimax 对状态赋值的方式如下：

\[
\forall \text{智能体控制的状态}, \quad
V(s) = \max_{s' \in successors(s)} V(s')
\]

\[
\forall \text{对手控制的状态}, \quad
V(s) = \min_{s' \in successors(s)} V(s')
\]

\[
\forall \text{终止状态}, \quad
V(s) = known
\]

在实现上，minimax 的行为类似于深度优先搜索（DFS），它按照 DFS 的顺序计算节点的价值，从最左侧的终止节点开始，逐步向右推进。更准确地说，它对博弈树执行后序遍历。minimax 的伪代码既优雅又直观简单，如下所示。请注意，minimax 将返回一个动作，该动作对应于根节点所选择的那个子节点分支。

![](../../image/3.Games/minimax-pseudocode.png)

---

### 3.2.1 Alpha-Beta Pruning

Minimax 看起来几乎完美——它简单、最优且直观。然而，它的执行方式与深度优先搜索非常相似，其时间复杂度也是一样的，令人沮丧的  
O(b^m)。回顾一下，b 是分支因子，m 是大约能找到终止节点的树深度。对于许多游戏来说，这会带来过高的运行时间。例如，国际象棋的分支因子约为  
b ≈ 35，树深度约为  
m ≈ 100。为缓解这一问题，minimax 有一种优化方法——alpha-beta 剪枝。

从概念上讲，alpha-beta 剪枝的思想是：如果你正在通过查看节点 n 的后继来确定其值，一旦你知道 n 的值最多只能等于其父节点的最优值，就可以停止继续查看。让我们通过一个例子来拆解这句看似复杂的话。请考虑如下博弈树，其中方形节点表示终止状态，向下的三角形表示最小化节点，向上的三角形表示最大化节点：

![](../../image/3.Games/alphabeta-example-pt1.png)

Alpha-Beta Example Part 1

让我们回顾一下 minimax 是如何得到这棵树的——它首先遍历值为 3、12 和 8 的节点，并将  
min(3, 12, 8) = 3  
赋给最左侧的最小化节点。然后，它将  
min(2, 4, 6) = 2  
赋给中间的最小化节点，并将  
min(14, 5, 2) = 2  
赋给最右侧的最小化节点，最后将  
max(3, 2, 2) = 3  
赋给根节点的最大化节点。

然而，如果我们深入思考这个情况，就会意识到：当我们访问到中间最小化节点的值为 2 的子节点时，就不再需要查看该最小化节点的其他子节点。为什么？因为我们已经看到中间最小化节点的一个子节点值为 2，因此无论其他子节点的值是多少，该最小化节点的值最多只能是 2。既然这一点已经确定，让我们再进一步思考——根节点的最大化节点正在比较左侧最小化节点的值 3，以及一个  
≤ 2  
的值，因此它必然会选择左侧最小化节点返回的 3，而不管中间最小化节点的其余子节点是什么值。这正是我们可以剪枝、不再查看中间最小化节点剩余子节点的原因：

![](../../image/3.Games/alphabeta-example-pt2.png)

Alpha-Beta Example Part 2

实现这种剪枝可以将运行时间降低到最优情况下的  
O(b^(m/2))，  
实际上相当于将我们“可求解”的深度翻倍。在实践中通常达不到这么理想，但通常可以多搜索至少一到两层。这仍然非常重要，因为能够提前思考 3 步的玩家往往优于只能提前思考 2 步的玩家。这样的剪枝正是带有 alpha-beta 剪枝的 minimax 算法所做的事情，其实现如下：

![](../../image/3.Games/alphabeta-pseudo.png)

Alpha-Beta Pseudocode

请花些时间将其与标准 minimax 的伪代码进行比较，并注意我们现在可以在不遍历所有后继的情况下提前返回。

---

## 3.2.2 Evaluation Functions

尽管 alpha-beta 剪枝可以增加我们能够可行运行 minimax 的深度，但这通常仍然远远不足以遍历大多数游戏的完整搜索树。因此，我们引入评估函数（evaluation functions），即接受一个状态并输出该节点真实 minimax 值估计值的函数。通常来说，这被简单理解为：一个好的评估函数会给“更好”的状态分配更高的值，而给“更差”的状态分配更低的值。评估函数广泛用于深度受限 minimax，在这种情况下，我们将达到最大可解深度的非终止节点视为终止节点，并根据精心设计的评估函数为它们赋予伪终止效用。由于评估函数只能给出非终止状态效用的估计值，因此在运行 minimax 时将不再保证最优行动。

在设计一个运行 minimax 的智能体时，通常会对评估函数的选择进行大量思考和实验。评估函数越好，智能体的行为就越接近最优。此外，在使用评估函数之前搜索更深层的树结构通常也会带来更好的结果——将评估计算“埋得更深”可以减轻对最优性的损害。这些函数在博弈中的作用与标准搜索问题中的启发式函数非常相似。

评估函数最常见的设计形式是特征的线性组合：

\[
Eval(s) = w_1 f_1(s) + w_2 f_2(s) + ... + w_n f_n(s)
\]

其中每个 \( f_i(s) \) 表示从输入状态 \( s \) 中提取的一个特征，每个特征都对应一个权重 \( w_i \)。特征是指我们可以从游戏状态中提取并赋予数值的某种元素。例如，在跳棋游戏中，我们可以构建一个包含 4 个特征的评估函数：智能体的兵数量、智能体的王数量、对手的兵数量、对手的王数量。然后根据它们的重要性大致选择合适的权重。在跳棋示例中，为智能体的兵和王选择正权重，为对手的兵和王选择负权重是合理的。此外，我们可能会认为王在跳棋中比兵更有价值，因此对应王的特征权重应当比兵的特征权重具有更大的绝对值。下面是一个符合上述特征和权重设想的评估函数示例：

\[
Eval(s) =
2 \cdot agent\_kings(s)
+ agent\_pawns(s)
- 2 \cdot opponent\_kings(s)
- opponent\_pawns(s)
\]

正如你所看到的，评估函数的设计可以非常自由，也不一定必须是线性的。例如，基于神经网络的非线性评估函数在强化学习应用中非常常见。最重要的一点是：评估函数应当尽可能频繁地为更好的局面赋予更高的分数。这可能需要大量的微调和实验，通过使用包含多种不同特征和权重的评估函数来测试智能体的表现。

