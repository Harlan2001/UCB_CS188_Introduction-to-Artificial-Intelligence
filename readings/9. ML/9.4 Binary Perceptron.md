# 9.4 二元感知机（Binary Perceptron）

很好，现在你已经了解了线性分类器是如何工作的，但我们如何构建一个好的分类器呢？在构建分类器时，你首先会得到数据，并且每个数据都带有正确的类别标签；我们称这部分数据为训练集。你通过在训练数据上评估分类器，将预测结果与训练标签比较，并不断调整分类器的参数，直到达到目标，从而构建分类器。

下面我们来探讨一个简单线性分类器的具体实现：二元感知机。感知机是一个二元分类器——尽管它可以扩展以处理两个以上的类别。二元感知机的目标是找到一个能够完美分隔训练数据的决策边界。换句话说，我们要寻找最优的权重——最好的  
\(w\)——使得任何乘以权重的训练样本都能被完美分类。

## 算法

感知机算法的工作流程如下：

1. 将所有权重初始化为 0：\(w = 0\)

2. 对于每个训练样本，其特征为  
\(f(x)\) 且真实类别标签为 \(y^* \in \{-1, +1\}\)，执行以下步骤：

### 2.1 使用当前权重对样本进行分类，设 \(y\) 为当前权重 \(w\) 预测的类别：

\[
y = classify(x) = 
\begin{cases} 
+1 & \text{如果 } h_w(x) = w^T f(x) > 0 \\ 
-1 & \text{如果 } h_w(x) = w^T f(x) < 0 
\end{cases}
\]

### 2.2 将预测标签 \(y\) 与真实标签 \(y^*\) 比较：

- 如果 \(y = y^*\)，则不做任何操作  
- 否则，如果 \(y \neq y^*\)，则更新权重：  
  \[
  w \leftarrow w + y^* f(x)
  \]

3. 如果在遍历完所有训练样本后无需更新权重（所有样本均预测正确），则终止。否则，重复步骤 2。

---

## 更新权重

让我们分析并理解更新权重的过程。回顾上面步骤 2b，当分类器正确时，不做任何修改。但当分类器错误时，权重向量更新如下：

\[
w \leftarrow w + y^* f(x)
\]

其中 \(y^*\) 是真实标签（取值为 1 或 -1），\(x\) 是被错误分类的训练样本。你可以将这个更新规则理解如下：

- **情况 1：将正类误分类为负类**  
  \[
  w \leftarrow w + f(x)
  \]

- **情况 2：将负类误分类为正类**  
  \[
  w \leftarrow w - f(x)
  \]

为什么这样有效？一种理解方法是将其看作一种平衡动作。误分类发生的原因要么是训练样本的激活值比应有的值小得多（导致情况 1），要么是激活值比应有的值大得多（导致情况 2）。

考虑情况 1，当激活值应为正却为负时，激活值过小。我们对 \(w\) 的调整应当修正这一点，使该训练样本的激活值增大。为了验证更新规则 \(w \leftarrow w + f(x)\) 是否有效，我们来看激活值如何变化：

\[
h_{w+f(x)}(x) = (w + f(x))^T f(x) = w^T f(x) + f(x)^T f(x) = h_w(x) + f(x)^T f(x)
\]

根据更新规则，新激活值增加了 \(f(x)^T f(x)\)，这是一个正数，因此说明我们的更新是合理的。激活值变大——更接近正值。同样逻辑可应用于激活值过大（应为负但为正）时的误分类。你会发现更新会使新激活值减少 \(f(x)^T f(x)\)，从而更接近正确分类。

那么，为什么要加或减样本特征呢？一个合理的解释是：得分不仅仅由权重决定，得分还取决于权重与样本的乘积。这意味着样本的某些特征对得分的贡献比其他特征大。例如，考虑一个真实标签为 \(y^* = -1\) 的训练样本 \(x\)：

\[
w^T = [2, 2, 2], \quad f(x) = \begin{bmatrix}4\\0\\1\end{bmatrix}, \quad h_w(x) = (2 \times 4) + (2 \times 0) + (2 \times 1) = 10
\]

我们知道权重需要减小，因为激活值应为负才能正确分类。但我们不希望所有权重都改变相同的量。你会发现样本的第一个元素 4 对得分 10 的贡献远大于第三个元素，而第二个元素没有贡献。因此，合理的权重更新应当使第一个权重大幅改变，第三个权重略微改变，而第二个权重保持不变。使用样本本身进行更新，正好满足了这个需求。

---

## 可视化

在下图中，\(f(x)\) 是一个正类样本 (\(y^* = +1\)) 的特征向量，但被误分类——它位于由“旧权重”定义的决策边界的错误一侧。将其加入权重向量会得到一个新的权重向量，使其与 \(f(x)\) 的夹角减小，并同时移动决策边界。在这个例子中，决策边界移动足够，使 \(x\) 被正确分类（注意，误分类不一定总被修正——这取决于权重向量的大小以及 \(f(x)\) 离边界的距离）。

![](../../image/9.%20ML/linear_classifier_fig7.png)
- 图 1：使用旧权重误分类 \(x\)  
![](../../image/9.%20ML/linear_classifier_fig8.png)
- 图 2：更新权重 \(w\)  
![](../../image/9.%20ML/linear_classifier_fig9.png)
- 图 3：更新后的 \(x\) 分类

---

## 偏置项（Bias）

如果按照前述方法实现感知机，你会注意到一个特别不方便的情况：生成的决策边界总是经过原点。也就是说，你的感知机只能生成形如 \(w^T f(x) = 0\) 的决策边界，其中 \(w, f(x) \in \mathbb{R}^n\)。问题是，即便数据中存在可以线性分隔正负类的边界，该边界可能不经过原点，而我们希望能够绘制任意线性边界。

为此，我们可以在特征和权重中添加偏置项：给样本特征向量添加一个始终为 1 的特征，同时在权重向量中增加一个对应权重。这样就可以生成形如：

\[
w^T f(x) + b = 0
\]

的决策边界，其中 \(b\) 是加权的偏置项（即权重向量最后一个元素乘以 1）。

几何上，我们可以通过比较激活函数 \(w^T f(x)\) 和带偏置的 \(w^T f(x) + b\) 的情况来理解。需要在特征空间上高一维来观察带偏置的情况。

![](../../image/9.%20ML/linear_classifier_fig10.png)
- 图 1：无偏置  
![](../../image/9.%20ML/linear_classifier_fig11.png)
- 图 2：有偏置

---

## 示例

下面给出一个逐步运行感知机算法的示例。

我们按照顺序遍历数据，使用感知机算法运行一次。初始权重向量为 \(w = [w_0, w_1, w_2] = [-1, 0, 0]\)，其中 \(w_0\) 是偏置特征的权重（始终对应特征 1）。

### 训练集

| # | f1 | f2 | \(y^*\) |
|---|----|----|---------|
| 1 | 1  | 1  | -       |
| 2 | 3  | 2  | +       |
| 3 | 2  | 4  | +       |
| 4 | 3  | 4  | +       |
| 5 | 2  | 3  | -       |

### 单次感知机权重更新过程

| step | Weights        | Score                           | Correct? | Update       |
|------|----------------|---------------------------------|----------|-------------|
| 1    | [-1, 0, 0]     | -1*1 + 0*1 + 0*1 = -1          | yes      | none        |
| 2    | [-1, 0, 0]     | -1*1 + 0*3 + 0*2 = -1          | no       | +[1, 3, 2] |
| 3    | [0, 3, 2]      | 0*1 + 3*2 + 2*4 = 14           | yes      | none        |
| 4    | [0, 3, 2]      | 0*1 + 3*3 + 2*4 = 17           | yes      | none        |
| 5    | [0, 3, 2]      | 0*1 + 3*2 + 2*3 = 12           | no       | -[1, 2, 3] |
| 6    | [-1, 1, -1]    |                                 |          |             |

到这里我们先停止，但实际上，该算法会对数据进行多次遍历，直到在单次遍历中所有数据点都被正确分类。

## 9.4.1 多类感知机（Multiclass Perceptron）

上面介绍的感知机是一个二元分类器，但我们可以相对容易地将其扩展到多类情况。主要区别在于权重的设置方式以及权重的更新方式。对于二元情况，我们有一个权重向量，其维度等于特征数（加上偏置特征）。对于多类情况，我们为每个类别都设置一个权重向量。因此，在三类情况下，我们有三个权重向量。为了对一个样本进行分类，我们计算每个类别的得分，即将特征向量与每个类别的权重向量做点积。得分最高的类别就是我们选择的预测类别。

例如，考虑三类情况。设样本的特征为 \(f(x) = [-2, 3, 1]\)，各类别 0、1 和 2 的权重为：

\[
w_0 = [-2, 2, 1]
\]  
\[
w_1 = [0, 3, 4]
\]  
\[
w_2 = [1, 4, -2]
\]

对每个类别做点积，得到得分：

\[
s_0 = 11, \quad s_1 = 13, \quad s_2 = 8
\]

因此我们预测 \(x\) 属于类别 1。

一个重要的点是，在实际实现中，我们不会将权重作为分开的结构存储，而是通常将它们堆叠成一个权重矩阵。这样，我们就不需要对每个类别单独做点积，而是通过一次矩阵-向量乘法完成。这在实践中通常效率更高（因为矩阵-向量乘法通常有高度优化的实现）。

在上面的例子中，这样表示为：

\[
W = 
\begin{bmatrix}
-2 & 2 & 1\\
0 & 3 & 4\\
1 & 4 & -2
\end{bmatrix}, \quad 
x = 
\begin{bmatrix}
-2\\
3\\
1
\end{bmatrix}
\]

预测类别为：

\[
\text{argmax}(Wx) = \text{argmax}([11, 13, 8]) = 1
\]

---

随着权重结构的变化，权重更新规则在多类情况下也有所不同。如果我们正确分类了样本，则与二元情况一样，不做任何操作。如果预测错误，例如预测类别 \(y \neq y^*\)，则对真实类别 \(y^*\) 的权重向量加上特征向量，对预测错误的类别 \(y\) 的权重向量减去特征向量。在上面的例子中，假设真实类别为类别 2，但我们预测为类别 1，则：

- 对类别 1 的权重向量减去 \(x\)：

\[
w_1 = [0, 3, 4] - [-2, 3, 1] = [2, 0, 3]
\]

- 对真实类别 2 的权重向量加上 \(x\)：

\[
w_2 = [1, 4, -2] + [-2, 3, 1] = [-1, 7, -1]
\]

这实际上是对“正确的权重向量进行奖励”，“对误导的、错误的权重向量进行惩罚”，而对其他权重向量保持不变。考虑了权重和权重更新的差异后，其余算法与二元情况基本相同：循环遍历每个样本，当出现错误时更新权重，直到不再犯错。

---

### 偏置项（Bias）

为了加入偏置项，方法与二元感知机相同——在每个特征向量中添加一个恒为 1 的额外特征，并在每个类别的权重向量中增加一个对应的权重（相当于矩阵形式中增加一列）。

