# 9.6 优化（Optimization）

线性回归方法允许我们通过对损失函数求导并将梯度设为零，得到权重的闭式解。然而，一般来说，对于某些目标函数，闭式解可能不存在。在这种情况下，我们使用基于梯度的方法来寻找最优权重。其基本思想是：梯度指向目标函数增长最快的方向。我们通过沿着最陡上升方向移动来最大化函数，通过沿着最陡下降方向移动来最小化函数。

梯度上升用于当目标是一个我们希望最大化的函数时。

### 算法 1：梯度上升（Gradient Ascent）

1. 随机初始化 \(w\)  
2. 当 \(w\) 未收敛时：
\[
w \leftarrow w + \alpha \nabla_w f(w)
\]

梯度下降用于当目标是一个我们希望最小化的损失函数时。注意，它与梯度上升的唯一区别是沿着梯度的反方向移动。

### 算法 2：梯度下降（Gradient Descent）

1. 随机初始化 \(w\)  
2. 当 \(w\) 未收敛时：
\[
w \leftarrow w - \alpha \nabla_w f(w)
\]

---

在开始时，我们随机初始化权重。学习率 \(\alpha\) 表示我们沿梯度方向移动的步长大小。对于机器学习中的大多数函数，很难给出一个最优学习率。实际上，我们希望学习率足够大，使得能够快速向正确方向移动，同时又足够小，以防算法发散。在机器学习文献中，一个常见做法是从相对较大的学习率开始梯度下降，并随着迭代次数增加逐步减小学习率（学习率衰减）。

---

如果数据集有大量 \(n\) 个数据点，那么在每次梯度下降迭代中计算上述梯度可能计算量过大。因此，提出了随机梯度下降（stochastic gradient descent）和批量梯度下降（batch gradient descent）的方法。在随机梯度下降中，每次迭代仅使用一个随机从数据集中采样的数据点来计算梯度。由于只使用一个数据点来估计梯度，随机梯度下降可能导致梯度噪声较大，从而使收敛变得更难。小批量梯度下降（mini-batch gradient descent）是随机梯度下降和普通梯度下降之间的折中方法，每次使用大小为 \(m\) 的数据批次计算梯度。批次大小 \(m\) 由用户指定。

---

让我们来看一个在之前见过的模型上应用梯度下降的例子——线性回归。回忆线性回归中，我们定义的损失函数为：

\[
\text{Loss}(h_w) = \frac{1}{2} \|y - X w\|_2^2
\]

线性回归有一个著名的闭式解：

\[
\hat{w} = (X^T X)^{-1} X^T y
\]

正如我们在上一节中看到的。然而，我们也可以选择通过梯度下降来求解最优权重。损失函数的梯度为：

\[
\nabla_w \text{Loss}(h_w) = -X^T y + X^T X w
\]

然后，我们使用该梯度写出线性回归的梯度下降算法：

### 算法 3：最小二乘梯度下降（Least Squares Gradient Descent）

1. 随机初始化 \(w\)  
2. 当 \(w\) 未收敛时：
\[
w \leftarrow w - \alpha (-X^T y + X^T X w)
\]

一个很好的练习是创建一个线性回归问题，并验证闭式解与梯度下降收敛得到的解是否相同。
