# 9.5 线性回归（Linear Regression）

现在我们将从之前讨论的朴素贝叶斯转向线性回归。该方法也称为最小二乘法，最早可追溯到卡尔·弗里德里希·高斯（Carl Friedrich Gauss），是机器学习和计量经济学中研究最为深入的工具之一。

回归问题是一种机器学习问题，其中输出是连续变量（记作 \(y\)）。特征可以是连续的或分类的。我们将一组特征表示为 \(x \in \mathbb{R}^n\)，其中 \(n\) 为特征数量，即

\[
x = (x_1, \dots, x_n)
\]

我们使用以下线性模型来预测输出：

\[
h_w(x) = w_0 + w_1 x_1 + \dots + w_n x_n
\]

其中线性模型的权重 \(w_i\) 是我们要估计的参数。权重 \(w_0\) 对应模型的截距项。有时在文献中，会在特征向量 \(x\) 中加上 1，使线性模型可以写作 \(w^T x\)，此时 \(x \in \mathbb{R}^{n+1}\)。

为了训练模型，我们需要一个度量模型预测输出的好坏的指标。为此，我们使用 \(L_2\) 损失函数，它使用 \(L_2\) 范数惩罚预测值与真实输出之间的差异。如果训练数据集有 \(N\) 个数据点，则损失函数定义如下：

\[
\begin{aligned}
\text{Loss}(h_w) &= \frac{1}{2N} \sum_{j=1}^N L_2(y_j, h_w(x_j)) \\
&= \frac{1}{2N} \sum_{j=1}^N (y_j - h_w(x_j))^2 \\
&= \frac{1}{2} \|y - X w\|_2^2
\end{aligned}
\]

注意 \(x_j\) 对应第 \(j\) 个数据点 \(x_j \in \mathbb{R}^n\)。这里的 \(\frac{1}{2}\) 只是为了简化闭式解的表达。最后一种形式是损失函数的等价表达，使得最小二乘法的推导更方便。向量 \(y\)、矩阵 \(X\) 和权重向量 \(w\) 定义如下：

\[
y = 
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}, \quad
X = 
\begin{bmatrix}
1 & x_1^1 & \dots & x_n^1 \\
1 & x_1^2 & \dots & x_n^2 \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_1^N & \dots & x_n^N
\end{bmatrix}, \quad
w = 
\begin{bmatrix}
w_0 \\
w_1 \\
\vdots \\
w_n
\end{bmatrix}
\]

其中 \(y\) 是堆叠输出的向量，\(X\) 是特征向量的矩阵，\(x_i^j\) 表示第 \(j\) 个数据点的第 \(i\) 个分量。

最小二乘解记为 \(\hat{w}\)，可以使用基础线性代数规则进行推导。更具体地说，我们通过对损失函数求导并令导数等于零来找到使损失函数最小的 \(\hat{w}\)：

\[
\begin{aligned}
\nabla_w \frac{1}{2} \|y - X w\|_2^2 &= \nabla_w \frac{1}{2} (y - X w)^T (y - X w) \\
&= \nabla_w \frac{1}{2} (y^T y - y^T X w - w^T X^T y + w^T X^T X w) \\
&= \nabla_w \frac{1}{2} (y^T y - 2 w^T X^T y + w^T X^T X w) \\
&= -X^T y + X^T X w
\end{aligned}
\]

将梯度设为零，我们得到：

\[
-X^T y + X^T X w = 0 \quad \Rightarrow \quad \hat{w} = (X^T X)^{-1} X^T y
\]

得到估计的权重向量后，我们就可以对新的未见过的测试数据点进行预测：

\[
\hat{h}_w(x) = \hat{w}^T x
\]
