# 9.2 朴素贝叶斯（Naive Bayes）

我们将通过一个具体的机器学习算法示例来引入机器学习的讨论。让我们考虑一个常见的问题——构建电子邮件垃圾邮件过滤器，它将邮件分类为垃圾邮件（不想要的邮件）或正常邮件（想要的邮件）。这样的问题称为分类问题——给定各种数据点（在此例中，每封电子邮件是一个数据点），我们的目标是将它们分组到两个或多个类别中的一个。对于分类问题，我们会得到一个带有对应标签的训练数据集，这些标签通常是几个离散值之一。

我们的目标是使用这些训练数据（电子邮件及每封邮件的垃圾邮件/正常邮件标签）来学习某种关系，以便对以前未见过的邮件进行预测。在本节中，我们将描述如何构建一种用于解决分类问题的模型，称为朴素贝叶斯分类器（Naive Bayes Classifier）。

![drawing](../../image/9.%20ML/robot.png)

为了训练一个将邮件分类为垃圾邮件或正常邮件的模型，我们需要一些经过预先分类的训练数据供学习使用。然而，电子邮件仅仅是文本字符串，为了学习有用的信息，我们需要从每封邮件中提取某些称为特征（features）的属性。特征可以是任何内容，从特定的单词计数到文本模式（例如，单词是否全部大写）到你能想象的几乎任何数据属性。

训练中提取的具体特征通常取决于你要解决的具体问题，而你选择使用哪些特征会显著影响模型性能。决定使用哪些特征被称为特征工程（feature engineering），这是机器学习的基础，但在本课程中，你可以假设给定的任何数据集都已经提供了提取好的特征。在本笔记中，\(f(x)\) 指的是应用于所有输入 \(x\) 的特征函数，然后将其输入模型。

现在假设你有一个包含 \(n\) 个单词的词典，从每封邮件中提取一个特征向量 \(F \in \mathbb{R}^n\)，其中 \(F\) 的第 \(i\) 个条目是随机变量 \(F_i\)，它的值可以为 0 或 1，取决于词典中第 \(i\) 个单词是否出现在待分类的邮件中。例如，如果 \(F_{200}\) 是单词 "free" 的特征，当邮件中出现 "free" 时，\(F_{200}=1\)；未出现时，\(F_{200}=0\)。

根据这些定义，我们可以更具体地定义如何预测邮件是垃圾邮件还是正常邮件——如果我们可以生成每个 \(F_i\) 与标签 \(Y\) 之间的联合概率表，就可以根据邮件的特征向量计算该邮件是垃圾邮件或正常邮件的概率。具体来说，我们可以计算：

\[
P(Y=\text{spam} \mid F_1=f_1, \dots, F_n=f_n)
\]  
\[
P(Y=\text{ham} \mid F_1=f_1, \dots, F_n=f_n)
\]  

然后根据两个概率中较大的值为邮件打标签。

不幸的是，由于我们有 \(n\) 个特征和 1 个标签，每个特征和标签可以取 2 个不同的值，因此对应该分布的联合概率表大小是 \(2^{n+1}\) 条目——非常不现实！这个问题可以通过用贝叶斯网络（Bayes’ net）建模联合概率表来解决，关键的简化假设是：给定类别标签，每个特征 \(F_i\) 与其他特征相互独立。

这是一个非常强的建模假设（这也是“朴素贝叶斯”名字中“朴素”的原因），但它简化了推理，并且在实践中通常效果良好。它生成了如下的贝叶斯网络，用于表示我们希望的联合概率分布。

![drawing](../../image/9.%20ML/nbmodel.png)

根据 \(d\)-分离规则可以清楚地看到，在这个贝叶斯网络中，每个 \(F_i\) 在给定 \(Y\) 的条件下与其他所有特征条件独立。现在我们有一个 \(P(Y)\) 的表格，包含 2 个条目，以及 \(n\) 个 \(P(F_i \mid Y)\) 的表格，每个表有 \(2^2=4\) 个条目，总共 \(4n+2\) 条目——关于 \(n\) 是线性的！这一简化假设体现了统计效率概念中的权衡；有时我们需要牺牲模型复杂度以保持计算可行性。

事实上，当特征数量足够少时，通常可以对特征之间的关系做更多假设，以生成更好的模型（对应于向贝叶斯网络中添加边）。使用此模型，对未知数据点进行预测相当于在贝叶斯网络上运行推理。我们已经观察到 \(F_1, \dots, F_n\) 的值，并希望选择具有最大条件概率的 \(Y\) 值：

\[
\text{prediction}(f_1, \dots, f_n) = \arg\max_y P(Y=y \mid F_1=f_1, \dots, F_n=f_n) = \arg\max_y P(Y=y, F_1=f_1, \dots, F_N=f_n) = \arg\max_y P(Y=y) \prod_{i=1}^{n} P(F_i=f_i \mid Y=y)
\]

第一步成立是因为归一化或未归一化分布下最大概率类别相同，第二步直接来源于朴素贝叶斯独立性假设，即给定类别标签，特征之间相互独立（如图模型所示）。

推广到一般情况，假设现在有 \(k\) 个类别标签（\(Y\) 的可能取值）。另外，注意到我们期望的概率——在给定特征的情况下每个标签 \(y_i\) 的概率

\[
P(Y=y_i \mid F_1=f_1, \dots, F_n=f_n)
\]

与联合概率

\[
P(Y=y_i, F_1=f_1, \dots, F_n=f_n)
\]

成正比，我们可以计算：

\[
P(Y, F_1 = f_1, \ldots, F_n = f_n) = 
\begin{bmatrix}
P(Y = y_1, F_1 = f_1, \ldots, F_n = f_n) \\
P(Y = y_2, F_1 = f_1, \ldots, F_n = f_n) \\
\vdots \\
P(Y = y_k, F_1 = f_1, \ldots, F_n = f_n)
\end{bmatrix}
\]
\[
\begin{bmatrix}
P(Y = y_1) \prod_i P(F_i = f_i \mid Y = y_1) \\
P(Y = y_2) \prod_i P(F_i = f_i \mid Y = y_2) \\
\vdots \\
P(Y = y_k) \prod_i P(F_i = f_i \mid Y = y_k)
\end{bmatrix}
\]

对于特征向量 \(F\) 的类别预测，只需选择上述向量中最大值对应的标签：

\[
\text{prediction}(F) = \arg\max_{y_i} P(Y=y_i) \prod_j P(F_j=f_j \mid Y=y_i)
\]

到此，我们已经学习了朴素贝叶斯分类器的基本建模假设理论以及如何用它进行预测，但如何从输入数据中学习贝叶斯网络所需的条件概率表还未涉及。这个内容将在下一节讨论，参数估计（parameter estimation）中进行。

## 9.2.1 参数估计

假设你有一组样本点或观察值 \(x_1, \dots, x_N\)，并且你认为这些数据是从一个由未知参数 \(\theta\) 描述的分布中抽取的。换句话说，你认为每个观察值 \(x_i\) 的概率 \(P_\theta(x_i)\) 是 \(\theta\) 的函数。例如，我们可能在抛一枚硬币，硬币正面朝上的概率为 \(\theta\)。

---

我们如何在给定样本的情况下“学习”最可能的 \(\theta\) 值？  
例如，如果我们抛了 10 次硬币，发现有 7 次正面，那么应该选择什么 \(\theta\)？  
一种方法是推断 \(\theta\) 为 **最大化已观察样本 \(x_1, \dots, x_N\) 从假设分布中抽取的概率** 的值。  
在机器学习中，一个常用且基础的方法叫做 **最大似然估计（MLE, Maximum Likelihood Estimation）** 就是做的这件事。

---

最大似然估计通常假设：

1. 每个样本都来自同一分布，即每个 \(x_i\) 同分布（i.i.d.，independent and identically distributed）。在抛硬币的例子中，每次抛硬币出现正面的概率都是 \(\theta\)。  
2. 每个样本 \(x_i\) 在给定分布参数的条件下，相互独立。这是一个强假设，但可以大大简化最大似然估计问题，并且通常在实践中表现良好。在抛硬币的例子中，一次抛硬币的结果不会影响其他抛掷结果。  
3. 在看到数据之前，所有可能的 \(\theta\) 值被认为是等可能的（均匀先验）。  

前两个假设称为 i.i.d.，第三个假设则将 MLE 看作 **最大后验估计（MAP）** 的特例，允许非均匀先验。

---

现在定义样本的似然函数 \(L(\theta)\)，它表示从该分布中抽取到样本的概率。对于固定样本 \(x_1, \dots, x_N\)，似然函数只是 \(\theta\) 的函数：

\[
L(\theta) = P_\theta(x_1, \dots, x_N)
\]

使用 i.i.d. 假设，可以改写为：

\[
L(\theta) = \prod_{i=1}^{N} P_\theta(x_i)
\]

---

如何找到使该函数最大化的 \(\theta\) 值？  
这个 \(\theta\) 值就是最能解释我们观察到的数据的参数。根据微积分的知识，函数在极值点的导数必须为零。因此，最大似然估计 \(\theta\) 满足：

\[
\frac{\partial}{\partial \theta} L(\theta) = 0
\]

---

### 示例

假设你有一个装有红球和蓝球的袋子，但不知道每种球的数量。  
你每次抽球后记下颜色再放回（有放回抽样）。  
抽三次得到的样本为：红、红、蓝。  
直观上，你可能推断袋子中 \(\frac{2}{3}\) 的球是红色，\(\frac{1}{3}\) 的球是蓝色。  

假设每次抽到红球的概率为 \(\theta\)，蓝球概率为 \(1-\theta\)（伯努利分布）：

\[
P_\theta(x_i) = 
\begin{cases}
\theta, & x_i = red \\
1-\theta, & x_i = blue
\end{cases}
\]

样本的似然函数为：

\[
\begin{aligned}
L(\theta) &= \prod_{i=1}^{3} P_\theta(x_i) \\
&= P_\theta(x_1=red) \cdot P_\theta(x_2=red) \cdot P_\theta(x_3=blue) \\
&= \theta^2 \cdot (1-\theta)
\end{aligned}
\]

---

对似然函数求导并令其为 0：

\[
\frac{\partial}{\partial \theta} L(\theta) = \frac{\partial}{\partial \theta} (\theta^2 \cdot (1-\theta)) = \theta(2 - 3\theta) = 0
\]

解方程得到 \(\theta = \frac{2}{3}\)，直观上完全合理！  
> 注意还有第二个解 \(\theta = 0\)，但对应似然函数的最小值 \(L(0) = 0 < L(2/3) = 4/27\)。

---


## 9.2.2 Naive Bayes 的最大似然估计

现在我们回到为垃圾邮件分类器推断条件概率表的问题，首先回顾我们已知的变量：

- \(n\) — 字典中的单词数量。  
- \(N\) — 训练中观察到的样本数量（电子邮件数量）。在接下来的讨论中，我们还定义 \(N_h\) 为标记为 ham 的训练样本数量，\(N_s\) 为标记为 spam 的训练样本数量。注意 \(N_h + N_s = N\)。  
- \(F_i\) — 随机变量，如果考虑中的电子邮件包含字典中的第 \(i\) 个单词，则为 1，否则为 0。  
- \(Y\) — 随机变量，取值为 spam 或 ham，取决于对应邮件的标签。  
- \(f_i^{(j)}\) — 表示训练集中第 \(j\) 个样本中随机变量 \(F_i\) 的已确定值。换句话说，如果第 \(i\) 个单词出现在第 \(j\) 封邮件中，则 \(f_i^{(j)}=1\)，否则为 0。这是我们第一次使用这种符号，但在接下来的推导中会很方便。

> 免责声明：如果不想看数学推导，可以直接看本节末段总结的结果。

---

在每个条件概率表 \(P(F_i|Y)\) 中，我们有两个不同的伯努利分布：  
\(P(F_i|Y=ham)\) 和 \(P(F_i|Y=spam)\)。  

为了简化讨论，我们先考虑 \(P(F_i|Y=ham)\)，尝试求其参数的最大似然估计：

\[
\theta = P(F_i=1 | Y=ham)
\]

即字典中第 \(i\) 个单词出现在 ham 邮件中的概率。因为训练集中有 \(N_h\) 封 ham 邮件，我们有 \(N_h\) 个关于该单词是否出现的观察值。由于模型假设每个单词在给定标签下服从伯努利分布，我们可以写出似然函数：

\[
L(\theta) = \prod_{j=1}^{N_h} P(F_i = f_i^{(j)} \mid Y=ham)
= \prod_{j=1}^{N_h} \theta^{f_i^{(j)}} (1-\theta)^{1 - f_i^{(j)}}
\]

解释：  
- 如果 \(f_i^{(j)}=1\)，则 \(P(F_i=f_i^{(j)}|Y=ham)=\theta^1 (1-\theta)^0 = \theta\)  
- 如果 \(f_i^{(j)}=0\)，则 \(P(F_i=f_i^{(j)}|Y=ham)=\theta^0 (1-\theta)^1 = 1-\theta\)

---

为了求最大似然估计 \(\theta\)，我们通常对 \(L(\theta)\) 求导并令其等于 0。但直接求解较困难，因此我们常用技巧：最大化 **对数似然函数**。因为 \(\log(x)\) 是严格递增的，最大化 \(\log L(\theta)\) 也能最大化 \(L(\theta)\)。展开对数似然：

\[
\begin{aligned}
\log L(\theta) &= \log \prod_{j=1}^{N_h} \theta^{f_i^{(j)}} (1-\theta)^{1 - f_i^{(j)}} \\
&= \sum_{j=1}^{N_h} \log \big( \theta^{f_i^{(j)}} (1-\theta)^{1 - f_i^{(j)}} \big) \\
&= \sum_{j=1}^{N_h} \log(\theta^{f_i^{(j)}}) + \sum_{j=1}^{N_h} \log((1-\theta)^{1 - f_i^{(j)}}) \\
&= \log(\theta) \sum_{j=1}^{N_h} f_i^{(j)} + \log(1-\theta) \sum_{j=1}^{N_h} (1 - f_i^{(j)})
\end{aligned}
\]

注意上面使用了对数运算性质：
\(\log(a^c) = c \cdot \log(a)\) 和 \(\log(ab) = \log(a)+\log(b)\)。

---

令对数似然的导数等于 0，求 \(\theta\)：

\[
\frac{\partial}{\partial \theta} 
\Big( \log(\theta) \sum_{j=1}^{N_h} f_i^{(j)} + \log(1-\theta) \sum_{j=1}^{N_h} (1-f_i^{(j)}) \Big) = 0
\]

展开：

\[
\frac{1}{\theta} \sum_{j=1}^{N_h} f_i^{(j)} - \frac{1}{1-\theta} \sum_{j=1}^{N_h} (1-f_i^{(j)}) = 0
\]

化简：

\[
\theta \sum_{j=1}^{N_h} (1-f_i^{(j)}) = \sum_{j=1}^{N_h} f_i^{(j)}
\]

最终得到：

\[
\theta = \frac{1}{N_h} \sum_{j=1}^{N_h} f_i^{(j)}
\]

---

### 总结

最大似然估计 \(\theta\) 的结果非常直观：  
- 对于每个 class（这里是 ham），统计出现某单词的邮件数量，除以该 class 的总邮件数。  
- 对 Naive Bayes 模型（伯努利特征）而言，任意特征在某个 class 下的概率 = 该特征出现次数 / 总样本数。  

> 该推导可推广到多 class 或多 outcome 的情况，只是这里没有详细展示。

## 9.2.3 平滑（Smoothing）

虽然最大似然估计是参数估计中非常强大的方法，但糟糕的训练数据常常会导致不良后果。  
例如，如果在训练集中，每次单词 “minute” 出现时，该邮件都被标记为垃圾邮件，那么训练出的模型将会学习到：

\[
P(F_{\text{minute}} = 1 \mid Y = \text{ham}) = 0
\]

因此，在未见过的邮件中，如果单词 minute 出现：

\[
P(Y = \text{ham}) \prod_i P(F_i \mid Y = \text{ham}) = 0
\]

于是你的模型将永远不会将任何包含单词 minute 的邮件分类为 ham（非垃圾邮件）。  
这是过拟合（overfitting）的经典例子，即建立了一个对未见数据泛化能力差的模型。  
仅仅因为某个单词在训练邮件中没有出现，并不意味着它在测试数据或真实世界邮件中不会出现。

---

对于朴素贝叶斯分类器的过拟合问题，可以通过 **拉普拉斯平滑（Laplace smoothing）** 来缓解。  
概念上，具有强度 \(k\) 的拉普拉斯平滑假设我们“额外看到”了每个可能结果 \(k\) 次。  
因此，对于给定样本，如果你针对可能取 \(|X|\) 个不同值的结果 \(x\) 的最大似然估计（MLE）为：

\[
P_{\text{MLE}}(x) = \frac{\text{count}(x)}{N}
\]

那么强度为 \(k\) 的拉普拉斯估计为：

\[
P_{\text{LAP}, k}(x) = \frac{\text{count}(x) + k}{N + k|X|}
\]

---

这个公式是什么意思？  
- 我们假设每个结果额外出现了 \(k\) 次，所以好像我们看到的是 \(\text{count}(x) + k\) 个实例，而不是原来的 \(\text{count}(x)\) 个。  
- 同样地，如果每个 \(|X|\) 个类别额外出现了 \(k\) 次，那么我们必须在原始样本数量 \(N\) 上加上 \(k|X|\)。  

这两个条件合起来，就得到了上面的公式。  

对于条件概率的拉普拉斯估计也类似（这对于计算不同类别下的结果概率很有用）：

\[
P_{\text{LAP}, k}(x \mid y) = \frac{\text{count}(x, y) + k}{\text{count}(y) + k|X|}
\]

---

拉普拉斯平滑有两个特别值得注意的极端情况：

1. 当 \(k = 0\) 时：

\[
P_{\text{LAP}, 0}(x) = P_{\text{MLE}}(x)
\]

2. 当 \(k = \infty\) 时：  
观察到每个结果的次数非常大（无限多），使得实际样本的结果无关紧要，拉普拉斯估计意味着每个结果是等概率的：

\[
P_{\text{LAP}, \infty}(x) = \frac{1}{|X|}
\]

---

在实际建模中，适合使用的 \(k\) 值通常通过 **试错法**（trial-and-error）确定。  
\(k\) 是模型中的一个 **超参数（hyperparameter）**，你可以设置不同的值，并观察在验证数据上哪一个值能得到最佳预测准确率或性能。
