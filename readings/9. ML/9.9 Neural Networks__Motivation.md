# 9.9 神经网络：动机（Neural Networks: Motivation）

在下面的内容中，我们将介绍神经网络的概念。在此过程中，我们会使用之前为二元逻辑回归和多类别逻辑回归建立的建模技术。

## 9.9.1 非线性分隔器（Non-linear Separators）

我们知道如何为二元分类任务构建一个学习线性边界的模型。这是一个强大的技术，当最优决策边界本身是线性时，这种方法效果很好。然而，许多实际问题需要非线性的决策边界，而我们的线性感知器模型不足以捕捉这种关系。

考虑以下数据集：

![](../../image/9.%20ML/nonlinsep.png)


我们希望分隔两种颜色，显然在单维空间中无法完成（单维决策边界只是一个点，将轴分成两个区域）。

为了解决这个问题，我们可以添加额外的（可能是非线性的）特征来构建决策边界。考虑在数据集中添加 \(x^2\) 作为特征后的情况：

![](../../image/9.%20ML/2dlinsep.png)


有了这个附加信息，我们现在可以在包含这些点的二维空间中构建线性分隔器。在这个例子中，我们通过手动为数据点添加有用特征，将数据映射到更高维空间，从而解决了问题。然而，在许多高维问题中，例如图像分类，手动选择有用特征是繁琐的，这需要领域特定的专业知识，并且不利于任务间的泛化。自然的想法是使用非线性函数类来学习这些特征变换，以表示更多种类的函数。

---

## 9.9.2 多层感知器（Multi-layer Perceptron）

让我们研究如何从原始感知器架构派生出更复杂的函数。考虑以下设置：二层感知器，它的输入是另一个感知器的输出。

![](../../image/9.%20ML/2dpercep.png)

实际上，我们可以将其推广到 N 层感知器：

![](../../image/9.%20ML/npercep.png)


通过增加这种结构和权重，我们可以表达更广泛的函数集合。

增加模型的复杂性会显著提高其表达能力。多层感知器提供了一种通用方法来表示更广泛的函数集合。事实上，多层感知器是一个**通用函数逼近器**，可以表示任意实函数，我们只需选择最佳权重来参数化网络。这一结论正式表述如下：

### 9.9.2.1 定理（Universal Function Approximators）

一个拥有足够数量神经元的二层神经网络可以以任意精度逼近任意连续函数。

---

## 9.9.3 准确率测量（Measuring Accuracy）

二元感知器在进行 \(n\) 次预测后的准确率可以表示为：

\[
\text{lacc}(w) = \frac{1}{n} \sum_{i=1}^n (\text{sgn}(w \cdot f(x_i)) == y_i)
\]

其中 \(x_i\) 是第 \(i\) 个数据点，\(w\) 是权重向量，\(f\) 是从原始数据点生成特征向量的函数，\(y_i\) 是 \(x_i\) 的真实类别标签。在这里，\(\text{sgn}(x)\) 是指标函数，当 \(x\) 为负时为 -1，正时为 1。准确率函数相当于正确预测的数量除以总预测次数。

有时我们希望输出比二元标签更有表达力，这时为每个 \(N\) 类生成概率是有用的，反映了数据点属于每个类别的置信度。如同多类别逻辑回归，我们为每个类别 \(j\) 存储一个权重向量，并用 softmax 函数估计概率：

\[
\sigma(x_i)_j = \frac{e^{f(x_i)^T w_j}}{\sum_{\ell=1}^N e^{f(x_i)^T w_\ell}} = P(y_i = j \mid f(x_i); w)
\]

softmax 对向量输出进行归一化，使其成为概率分布。为了为模型导出通用损失函数，我们可以使用这个概率分布生成权重集的似然表达式：

\[
\ell(w) = \prod_{i=1}^n P(y_i \mid f(x_i); w)
\]

该表达式表示特定权重集解释观测标签和数据点的可能性。我们寻找能够最大化该值的权重集，这等价于最大化对数似然：

\[
\log \ell(w) = \log \prod_{i=1}^n P(y_i \mid x_i; w) = \sum_{i=1}^n \log P(y_i \mid f(x_i); w)
\]

---

## 9.9.4 多层前馈神经网络（Multi-layer Feedforward Neural Networks）

我们现在引入人工神经网络的概念。与多层感知器类似，我们选择在每个感知器节点输出后应用非线性函数。这些非线性函数使网络整体成为非线性、更具表达力的模型。没有它们，多层感知器仅是线性函数的组合，因此仍然是线性的。

在多层感知器中，我们选择了阶跃函数：

\[
f(x) =
\begin{cases}
1 & x \ge 0 \\
-1 & x < 0
\end{cases}
\]

图形如下：

![](../../image/9.%20ML/step_function.png)


该函数难以优化，因为它不连续，并且在所有点的导数为 0。一个更好的选择是连续函数，例如 Sigmoid 函数或修正线性单元（ReLU）。

### 9.9.4.1 Sigmoid 函数

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

![](../../image/9.%20ML/sigmoid_function.png)


### 9.9.4.2 ReLU 函数

\[
f(x) =
\begin{cases}
0 & x < 0 \\
x & x \ge 0
\end{cases}
\]

![](../../image/9.%20ML/relu_function.png)


在多层感知器中，我们在每一层输出应用非线性函数。选择哪种非线性通常是一个设计决策，需要实验验证。

---

## 9.9.5 损失函数与多变量优化（Loss Functions and Multivariate Optimization）

理解了前馈神经网络的构建方式后，我们需要一种训练方法。回到对数似然函数，我们可以推导出优化权重的直观算法。

为了最大化对数似然函数，我们对其求导以获得梯度向量：

\[
\nabla_w \ell(w) = \left[ \frac{\partial \ell(w)}{\partial w_1}, \dots, \frac{\partial \ell(w)}{\partial w_n} \right]
\]

我们使用梯度上升法找到参数的最优值。由于数据集通常较大，批量梯度上升（batch gradient ascent）是神经网络优化中最常用的梯度上升变体。
