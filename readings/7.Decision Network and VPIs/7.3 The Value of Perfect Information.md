### 7.3 完美信息的价值（Value of Perfect Information, VPI）

在此前的讨论中，我们通常假设代理人掌握了解决特定问题所需的全部信息，或者无法获取新信息。但实际上，决策中一个核心问题是：**是否值得收集更多证据来决定采取哪种行动**。观察新证据几乎总有成本，包括时间、金钱或其他资源。

完美信息的价值（VPI）数学上量化了**观察某些新证据后，代理人最大期望效用（MEU）预期增加的幅度**。通过比较 VPI 与观察成本，可以判断是否值得获取新信息。

---

#### 7.3.1 通用公式

完美信息的价值定义为观察新证据后 MEU 的增加幅度。假设当前证据为 \(e\)，当前最大期望效用为：

\[
MEU(e) = \max_a \sum_s P(s \mid e) U(s, a)
\]

如果在采取行动前观察到新证据 \(e'\)，则最大期望效用为：

\[
MEU(e, e') = \max_a \sum_s P(s \mid e, e') U(s, a)
\]

但我们事先不知道将观察到什么新证据，因此将其视为随机变量 \(E'\)：

\[
MEU(e, E') = \sum_{e'} P(e' \mid e) MEU(e, e')
\]

完美信息的价值定义为：

\[
\boxed{VPI(E' \mid e) = MEU(e, E') - MEU(e)}
\]

即“在当前证据 \(e\) 下，观察新证据 \(E'\) 的价值”。

---

#### 示例：天气情景

![](../../image/7.%20Decision%20Network%20and%20VPIs/vpi-example.png)

假设没有观察任何证据时：

\[
\begin{aligned}
MEU(\varnothing) &= \max_a EU(a) \\
&= \max_a \sum_w P(w) U(a, w) \\
&= \max\{0.7 \cdot 100 + 0.3 \cdot 0, \ 0.7 \cdot 20 + 0.3 \cdot 70\} \\
&= \max\{70, 35\} \\
&= 70
\end{aligned}
\]

若考虑观察天气预报 \(F\)：

\[
MEU(F=\text{bad}) = 53, \quad MEU(F=\text{good}) = 95
\]

则：

\[
\begin{aligned}
MEU(\varnothing, F) &= \sum_{f} P(F=f) \, MEU(F=f) \\
&= 0.59 \cdot 95 + 0.41 \cdot 53 \\
&= 77.78
\end{aligned}
\]

因此：

\[
VPI(F) = MEU(\varnothing, F) - MEU(\varnothing) = 77.78 - 70 = \boxed{7.78}
\]

---

#### 7.3.2 VPI 的属性

完美信息价值具有几个重要属性：

1. **非消极性（Non-negativity）**  
   \[
   \forall E', e:\ VPI(E' \mid e) \ge 0
   \]  
   观察新信息总能增加 MEU（或至少不减少）。

2. **非加性（Non-additivity）**  
   \[
   VPI(E_j, E_k \mid e) \neq VPI(E_j \mid e) + VPI(E_k \mid e)
   \]  
   因为观察 \(E_j\) 可能改变我们对 \(E_k\) 的关注程度，不能简单叠加 VPI。

3. **秩序独立性（Order-independence）**  
   \[
   VPI(E_j, E_k \mid e) = VPI(E_j \mid e) + VPI(E_k \mid e, E_j) = VPI(E_k \mid e) + VPI(E_j \mid e, E_k)
   \]  
   无论观察多个新证据的顺序如何，最大期望效用增益不变。
