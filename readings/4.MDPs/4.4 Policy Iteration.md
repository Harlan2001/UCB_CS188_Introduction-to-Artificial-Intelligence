## 4.4 策略迭代（Policy Iteration）

值迭代（Value Iteration）可能会非常慢。在每一次迭代中，我们必须更新所有 \(|S|\) 个状态的值（这里 \(|S|\) 表示状态集合的基数），每个状态都需要对所有 \(|A|\) 个动作进行迭代，以计算每个动作的 Q 值。而每个 Q 值的计算又需要再次对所有 \(|S|\) 个状态迭代，导致运行时间复杂度为：

$$
O(|S|^2 |A|)
$$

此外，当我们仅想确定 MDP 的最优策略时，值迭代往往会做大量多余的计算，因为通过策略提取得到的策略通常比状态值本身收敛得快得多。为了解决这些问题，可以使用 **策略迭代（Policy Iteration）** 作为替代算法，它保持了值迭代的最优性，同时提供显著的性能提升。策略迭代的操作步骤如下：

1. **定义初始策略**。初始策略可以是任意的，但初始策略越接近最终最优策略，策略迭代收敛得越快。

2. **重复以下步骤直到收敛**：

   1. **策略评估（Policy Evaluation）**  
      对当前策略进行评估。对于策略 \(\pi\)，策略评估意味着计算所有状态 \(s\) 的值 \(V^\pi(s)\)，其中 \(V^\pi(s)\) 表示从状态 \(s\) 开始并按照策略 \(\pi\) 行动时的期望效用：

      $$
      V^\pi(s) = \sum_{s'} T(s, \pi(s), s') \big[ R(s, \pi(s), s') + \gamma V^\pi(s') \big]
      $$

      设策略迭代第 \(i\) 次的策略为 \(\pi_i\)。由于每个状态只固定一个动作，因此不再需要 \(\max\) 运算，相当于生成了 \(|S|\) 个方程的线性方程组。可以通过直接求解该方程组得到 \(V^{\pi_i}(s)\)。  

      或者，也可以像值迭代一样使用以下更新规则迭代至收敛：

      $$
      V^{\pi_i}_{k+1}(s) \gets \sum_{s'} T(s, \pi_i(s), s') \big[ R(s, \pi_i(s), s') + \gamma V^{\pi_i}_k(s') \big]
      $$

      不过在实践中，这种方法通常较慢。

   2. **策略改进（Policy Improvement）**  
      一旦评估了当前策略，就可以使用策略改进生成一个更优的策略。策略改进通过对策略评估得到的状态值进行策略提取来生成新策略：

      $$
      \pi_{i+1}(s) = \arg\max_a \sum_{s'} T(s,a,s') \big[ R(s,a,s') + \gamma V^{\pi_i}(s') \big]
      $$

      如果 \(\pi_{i+1} = \pi_i\)，算法收敛，可以得出 \(\pi_{i+1} = \pi_i = \pi^*\)。

---

### Racecar 示例

![](../../image/4.MDPs/race-car.png)

我们用之前的赛道赛车 MDP 再次演示（折扣因子 \(\gamma=0.5\)）。  

初始策略：**总是慢速（Always go slow）**

|   | cool | warm | overheated |
|---|------|------|------------|
| \(\pi_0\) | slow | slow | — |

由于终止状态没有可行动作，无法给这些状态分配策略值，因此可以忽略 `overheated` 状态，并规定任意终止状态 \(s\) 的值为：

$$
\forall i, \quad V^{\pi_i}(s) = 0
$$

#### 第一次策略评估（\(\pi_0\)）：

$$
\begin{aligned}
V^{\pi_0}(cool) &= 1 \cdot [1 + 0.5 \cdot V^{\pi_0}(cool)] \\
V^{\pi_0}(warm) &= 0.5 \cdot [1 + 0.5 \cdot V^{\pi_0}(cool)] + 0.5 \cdot [1 + 0.5 \cdot V^{\pi_0}(warm)]
\end{aligned}
$$

解该方程组，得到：

|   | cool | warm | overheated |
|---|------|------|------------|
| \(V^{\pi_0}\) | 2 | 2 | 0 |

#### 第一次策略改进：

$$
\begin{aligned}
\pi_1(cool) &= \arg\max\{
\text{slow}: 1 \cdot [1 + 0.5 \cdot 2],\ 
\text{fast}: 0.5 \cdot [2 + 0.5 \cdot 2] + 0.5 \cdot [2 + 0.5 \cdot 2]
\} = \text{fast} \\
\pi_1(warm) &= \arg\max\{
\text{slow}: 0.5 \cdot [1 + 0.5 \cdot 2] + 0.5 \cdot [1 + 0.5 \cdot 2],\ 
\text{fast}: 1 \cdot [-10 + 0.5 \cdot 0]
\} = \text{slow}
\end{aligned}
$$

#### 第二次策略迭代：

得到 \(\pi_2(cool) = \text{fast}\)，\(\pi_2(warm) = \text{slow}\)。  

由于 \(\pi_2 = \pi_1\)，因此收敛：

$$
\pi^* = \pi_1 = \pi_2
$$

|   | cool | warm |
|---|------|------|
| \(\pi_0\) | slow | slow |
| \(\pi_1\) | fast | slow |
| \(\pi_2\) | fast | slow |

---

通过策略迭代，我们只用了两次迭代就得到赛道赛车 MDP 的最优策略！  
相比之下，值迭代在执行相同更新两次后，离收敛仍有若干步距离。
