## 4.2 求解马尔可夫决策过程

回忆一下，在确定性的、非对抗性的搜索问题中，求解一个搜索问题意味着找到一个到达目标状态的最优计划。而求解一个马尔可夫决策过程，则意味着找到一个最优策略  
π\* : S → A，即一个将每个状态 s ∈ S 映射到某个动作 a ∈ A 的函数。一个显式策略 π 定义了一个反射型智能体——给定一个状态 s，处于 s 并执行 π 的智能体会选择 a = π(s) 作为要执行的动作，而不去考虑其行为在未来的后果。一个最优策略是指：如果智能体按照该策略行动，将获得最大期望总回报或效用。

考虑如下 MDP，其中  
S = {a, b, c, d, e}，  
A = {East, West, Exit}（其中 Exit 仅在状态 a 和 e 中为合法动作，并分别产生 10 和 1 的奖励），折扣因子 γ = 0.1，且转移是确定性的：

![](../../image/4.MDPs/easy-mdp.png)

Easy MDP

该 MDP 的两个可能策略如下：

![](../../image/4.MDPs/policy-1.png)  
Policy 1  
![](../../image/4.MDPs/policy-2.png)
Policy 2  

经过一些分析，不难确定 Policy 2 是最优的。按照该策略执行，直到采取动作 a = Exit 时，从每个起始状态得到的奖励如下：

Start State  Reward  
a  10  
b  1  
c  0.1  
d  0.1  
e  1  

接下来，我们将学习如何使用马尔可夫决策过程的贝尔曼方程，以算法方式求解此类 MDP（以及更复杂的 MDP）。

---

### 4.2.1 贝尔曼方程

为了讨论 MDP 的贝尔曼方程，我们首先需要引入两个新的数学量：

- 状态 s 的最优值 V\*(s) —— s 的最优值是指：一个从 s 出发并始终采取最优行为的智能体，在其余生命周期中所能获得的效用的期望值。注意，在文献中该量通常记为 V\*(s)。

- Q 状态 (s, a) 的最优值 Q\*(s, a) —— (s, a) 的最优值是指：一个从 s 出发，先执行动作 a，然后此后始终采取最优行为的智能体所能获得的效用的期望值。

利用这两个新量以及前面讨论过的其他 MDP 量，贝尔曼方程定义如下：

V\*(s) = maxₐ  ∑ₛ′ T(s, a, s′) [ R(s, a, s′) + γ V\*(s′) ]

在解释其含义之前，我们也定义 Q 状态的最优值（更常称为最优 Q 值）的方程：

Q\*(s, a) = ∑ₛ′ T(s, a, s′) [ R(s, a, s′) + γ V\*(s′) ]

注意，这个定义使我们可以将贝尔曼方程改写为：

V\*(s) = maxₐ Q\*(s, a)

这是一个大大简化了的表达式。贝尔曼方程是动态规划方程的一个例子，即通过内在的递归结构将问题分解为更小子问题的方程。我们可以在 Q 值的公式中看到这种递归性，具体体现在项  
[ R(s, a, s′) + γ V\*(s′) ] 中。

该项表示：智能体从 s 执行动作 a 到达 s′ 后，并在此后始终采取最优行为所获得的总效用。动作 a 带来的即时奖励 R(s, a, s′)，加上从 s′ 出发能够获得的最优折扣奖励总和 V\*(s′)，其中乘以 γ 是为了反映执行动作 a 所消耗的一个时间步所带来的折扣。尽管从 s′ 到某个终止状态通常存在大量可能的状态—动作序列，但所有这些细节都被抽象并封装进单一的递归值 V\*(s′) 中。

现在我们可以进一步考虑 Q 值的完整公式。既然  
[ R(s, a, s′) + γ V\*(s′) ]  
表示从 Q 状态 (s, a) 到达状态 s′ 后继续最优行动所获得的效用，那么下式：

∑ₛ′ T(s, a, s′) [ R(s, a, s′) + γ V\*(s′) ]

本质上就是一个加权效用和，其中每个效用按照其发生的概率进行加权。这正是从 Q 状态 (s, a) 开始最优行动的期望效用！

这使我们能够完整理解贝尔曼方程——状态 s 的最优值 V\*(s)，就是从 s 出发在所有可能动作中所能获得的最大期望效用。计算状态 s 的最大期望效用，本质上与运行 expectimax 类似——我们首先计算每个 Q 状态 (s, a) 的期望效用（相当于计算机会节点的值），然后在这些值中取最大值（相当于计算最大化节点的值）。

关于贝尔曼方程的最后一点说明——它的作用是作为最优性的判定条件。换句话说，如果我们能够为每个状态 s ∈ S 找到一个值 V(s)，使得贝尔曼方程在这些状态上均成立，那么就可以断定这些值是对应状态的最优值。也就是说，满足该条件意味着：

∀ s ∈ S，V(s) = V\*(s)。
