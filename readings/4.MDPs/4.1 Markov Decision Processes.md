## 4.1 Markov 决策过程（Markov Decision Processes）

一个 Markov 决策过程由以下几个要素定义：

- 一组状态 \( S \)。MDP 中的状态表示方式与传统搜索问题中的状态相同。
- 一组动作 \( A \)。MDP 中的动作表示方式也与传统搜索问题中的动作相同。
- 一个起始状态。
- 可能存在一个或多个终止状态。
- 可能存在一个折扣因子 \( \gamma \)。我们很快会介绍折扣因子。
- 一个转移函数 \( T(s,a,s') \)。由于我们引入了非确定性动作的可能性，我们需要一种方式来刻画：从任意给定状态执行任意给定动作后，各种可能结果发生的概率。MDP 的转移函数正是完成这一任务——它是一个概率函数，表示智能体在状态 \( s \in S \) 下执行动作 \( a \in A \) 后，转移到状态 \( s' \in S \) 的概率。
- 一个奖励函数 \( R(s,a,s') \)。通常，MDP 会在每一步给予小的“存活”奖励以奖励智能体的持续生存，并在到达终止状态时给予较大的奖励。奖励可以是正值或负值，取决于它是否对智能体有利，而智能体的目标自然是在到达某个终止状态之前获得尽可能大的累计奖励。

为某种情境构建一个 MDP 与为搜索问题构建状态空间图非常相似，只是有一些额外注意事项。考虑一个赛车的示例：

![](../../image/4.MDPs/race-car.png)

Racecar Example

共有三个可能的状态  
\( S = \{ cool, warm, overheated \} \)，  
以及两个可能的动作  
\( A = \{ slow, fast \} \)。  

与状态空间图类似，这三个状态都由节点表示，边表示动作。状态 *overheated* 是一个终止状态，因为一旦赛车智能体到达该状态，就无法再执行任何动作以获取更多奖励（它是 MDP 中的汇点，没有出边）。需要注意的是，对于非确定性动作，在同一个状态下执行同一个动作可能会有多条边，对应不同的后继状态。每条边不仅标注了其对应的动作，还标注了转移概率和相应的奖励。如下所示：

### 转移函数：\( T(s,a,s') \)

\[
T(cool, slow, cool) = 1
\]

\[
T(warm, slow, cool) = 0.5
\]

\[
T(warm, slow, warm) = 0.5
\]

\[
T(cool, fast, cool) = 0.5
\]

\[
T(cool, fast, warm) = 0.5
\]

\[
T(warm, fast, overheated) = 1
\]

### 奖励函数：\( R(s,a,s') \)

\[
R(cool, slow, cool) = 1
\]

\[
R(warm, slow, cool) = 1
\]

\[
R(warm, slow, warm) = 1
\]

\[
R(cool, fast, cool) = 2
\]

\[
R(cool, fast, warm) = 2
\]

\[
R(warm, fast, overheated) = -10
\]

我们用离散时间步来表示智能体在不同 MDP 状态之间随时间的移动，定义  
\( s_t \in S \) 为智能体在时间步 \( t \) 所处的状态，  
\( a_t \in A \) 为智能体在时间步 \( t \) 所执行的动作。  

智能体在时间步 0 从状态 \( s_0 \) 开始，并在每个时间步执行一个动作。智能体在 MDP 中的移动过程可以表示为：

\[
s_0 \xrightarrow{a_0} s_1 \xrightarrow{a_1} s_2 \xrightarrow{a_2} s_3 \xrightarrow{a_3} \dots
\]

此外，由于智能体的目标是在所有时间步中最大化其奖励，我们可以将其数学表达为最大化如下效用函数：

\[
U([s_0,a_0,s_1,a_1,s_2,\dots]) =
R(s_0,a_0,s_1)
+
R(s_1,a_1,s_2)
+
R(s_2,a_2,s_3)
+
\dots
\]

Markov 决策过程与状态空间图类似，也可以展开为搜索树。在这些搜索树中，不确定性通过 Q 状态（也称为动作状态）来建模，其本质与 expectimax 的机会节点相同。这是一种合理的选择，因为 Q 状态通过概率来建模环境将智能体带入某个状态的不确定性，就像 expectimax 的机会节点通过概率建模对抗性智能体的行动不确定性一样。从状态 \( s \) 执行动作 \( a \) 所对应的 Q 状态记为元组 \( (s,a) \)。

观察我们赛车示例展开到深度 2 的搜索树：

![](../../image/4.MDPs/rc-search-tree.png)

Racecar Search Tree

绿色节点表示 Q 状态，即已经从某个状态执行了动作，但尚未解析为具体后继状态。需要理解的是，智能体在 Q 状态中停留的时间步为 0，它们只是为了便于表示和开发 MDP 算法而引入的构造。

---

## 4.1.1 有限视界与折扣

我们的赛车 MDP 存在一个内在问题——我们尚未对赛车可以执行动作并收集奖励的时间步数施加任何限制。按照当前定义，它可以在每个时间步永远选择 \( a = slow \)，安全且稳定地获得无限奖励，而没有任何过热风险。为防止这种情况，我们引入有限视界（finite horizons）和/或折扣因子。

带有限视界的 MDP 很简单——它为智能体定义一个“寿命”，给予其固定数量 \( n \) 个时间步来尽可能多地积累奖励，然后自动终止。我们稍后会再次讨论这一概念。

折扣因子则稍微复杂一些，它用于建模奖励随时间呈指数衰减的情况。具体而言，在折扣因子为 \( \gamma \) 的情况下，在时间步 \( t \) 从状态 \( s_t \) 执行动作 \( a_t \) 并到达状态 \( s_{t+1} \)，所获得的奖励为：

\[
\gamma^t R(s_t,a_t,s_{t+1})
\]

而不是仅仅：

\[
R(s_t,a_t,s_{t+1})
\]

此时，我们不再最大化简单的加性效用：

\[
U([s_0,a_0,s_1,a_1,s_2,\dots]) =
R(s_0,a_0,s_1)
+
R(s_1,a_1,s_2)
+
R(s_2,a_2,s_3)
+
\dots
\]

而是最大化折扣效用：

\[
U([s_0,a_0,s_1,a_1,s_2,\dots]) =
R(s_0,a_0,s_1)
+
\gamma R(s_1,a_1,s_2)
+
\gamma^2 R(s_2,a_2,s_3)
+
\dots
\]

注意到上述折扣效用函数的定义类似于公比为 \( \gamma \) 的几何级数，我们可以证明：只要满足约束 \( |\gamma| < 1 \)（其中 \( |n| \) 表示绝对值运算符），其值必然是有限的。证明如下：

\[
\begin{aligned}
U([s_0,s_1,s_2,\dots]) 
&= \sum_{t=0}^{\infty} \gamma^t R(s_t,a_t,s_{t+1}) \\
&\le \sum_{t=0}^{\infty} \gamma^t R_{\max} \\
&= \frac{R_{\max}}{1-\gamma}
\end{aligned}
\]

其中 \( R_{max} \) 是在任意时间步中可能获得的最大奖励。通常，\( \gamma \) 严格选自区间 \( 0 < \gamma < 1 \)，因为区间 \( -1 < \gamma \le 0 \) 的值在大多数现实场景中没有意义——当 \( \gamma \) 为负时，状态奖励将在交替时间步之间在正负之间来回翻转。

---

## 4.1.2 Markov 性

Markov 决策过程之所以称为“Markov”的，是因为它满足 Markov 性质（或无记忆性质），即：在给定当前状态的条件下，未来与过去条件独立。直观地说，如果我们已知当前状态，那么了解过去不会为我们提供关于未来的额外信息。

用数学表达如下。设某智能体在某个 MDP 中依次访问了状态  
\( s_0, s_1, \dots, s_t \)，  
并执行了动作  
\( a_0, a_1, \dots, a_{t-1} \)，  
且刚刚执行了动作 \( a_t \)。  

那么，该智能体到达状态 \( s_{t+1} \) 的概率可以表示为：

\[
P(S_{t+1}=s_{t+1} \mid
S_t=s_t,
A_t=a_t,
S_{t-1}=s_{t-1},
A_{t-1}=a_{t-1},
\dots,
S_0=s_0)
\]

其中每个 \( S_t \) 表示智能体在时间 \( t \) 的状态随机变量，  
每个 \( A_t \) 表示智能体在时间 \( t \) 执行的动作随机变量。

Markov 性质指出，上述概率可以简化为：

\[
P(S_{t+1}=s_{t+1} \mid S_t=s_t, A_t=a_t)
\]

即“无记忆”的含义：在时间 \( t+1 \) 到达状态 \( s' \) 的概率只依赖于时间 \( t \) 的状态 \( s \) 和动作 \( a \)，而不依赖于更早的状态或动作。事实上，正是这些无记忆概率被编码在转移函数中：

\[
T(s,a,s') = P(s' \mid s,a)
\]
