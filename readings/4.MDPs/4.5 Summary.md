## 4.5 总结（Summary）

上文介绍的内容容易引起混淆。我们讨论了值迭代（Value Iteration）、策略迭代（Policy Iteration）、策略提取（Policy Extraction）和策略评估（Policy Evaluation），这些方法看起来都很相似，都使用了贝尔曼方程（Bellman Equation），但存在一些细微差别。

下面是各算法用途的总结：

- **值迭代（Value Iteration）**：用于计算状态的最优值，通过迭代更新直至收敛。
- **策略评估（Policy Evaluation）**：用于计算在特定策略下的状态值。
- **策略提取（Policy Extraction）**：用于根据状态值函数确定策略。如果状态值是最优的，则提取出的策略也是最优策略。该方法通常在运行值迭代后使用，从最优状态值计算出最优策略；或者作为策略迭代的子程序，用于计算当前估计状态值下的最优策略。
- **策略迭代（Policy Iteration）**：结合策略评估和策略提取，用于迭代收敛到最优策略。相比值迭代，它通常表现更好，因为策略往往比状态值收敛得更快。
