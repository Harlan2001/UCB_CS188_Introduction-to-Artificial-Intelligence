## 4.3 值迭代

既然我们已经有了一个框架来检验 MDP 中状态价值的最优性，接下来的自然问题是：如何实际计算这些最优价值？为了解决这个问题，我们需要**时间受限价值**（即施加有限时域所得到的自然结果）。状态 $s$ 在时间上限为 $k$ 个时间步时的时间受限价值记为 $V_k(s)$，它表示在所考虑的马尔可夫决策过程将在 $k$ 个时间步后终止的前提下，从 $s$ 出发所能获得的最大期望效用。等价地，这就是在 MDP 搜索树上运行深度为 $k$ 的 expectimax 所得到的结果。

值迭代是一种动态规划算法，它通过逐步增加时间上限来计算时间受限价值，直到收敛（即对于每个状态，$V$ 的值与上一次迭代相同：$\forall s,\; V_{k+1}(s) = V_k(s)$）。其操作如下：

$\forall s \in S$，初始化  
$$
V_0(s) = 0.
$$

这是直观的，因为当时间上限为 0 个时间步时，在终止前无法采取任何行动，因此无法获得任何奖励。

重复以下更新规则直到收敛：

$$
\forall s \in S,\quad
V_{k+1}(s)
\leftarrow
\max_a
\sum_{s'} T(s,a,s')
\big[
R(s,a,s') + \gamma V_k(s')
\big]
$$

在值迭代的第 $k$ 次迭代中，我们使用时间上限为 $k$ 的时间受限价值来生成时间上限为 $k+1$ 的时间受限价值。本质上，我们利用已计算出的子问题解（所有的 $V_k(s)$）来逐步构建更大子问题的解（所有的 $V_{k+1}(s)$）；这正是值迭代成为动态规划算法的原因。

请注意，虽然贝尔曼方程在形式上与上述更新规则几乎相同，但它们并不相同。贝尔曼方程给出了最优性的一个条件，而更新规则则提供了一种迭代更新数值直到收敛的方法。当收敛时，贝尔曼方程对每个状态都成立：

$$
\forall s \in S,\quad
V_k(s) = V_{k+1}(s) = V^*(s).
$$

为了简洁起见，我们经常将

$$
U_{k+1}(s)
\leftarrow
\max_a
\sum_{s'} T(s,a,s')
\big[
R(s,a,s') + \gamma V_k(s')
\big]
$$

简写为

$$
V_{k+1} \leftarrow B U_k,
$$

其中 $B$ 被称为**贝尔曼算子**。贝尔曼算子是一个以 $\gamma$ 为收缩因子的收缩映射。为了证明这一点，我们需要以下一般不等式：

$$
\left|
\max_z f(z) - \max_z h(z)
\right|
\le
\max_z
\left|
f(z) - h(z)
\right|.
$$

现在考虑在同一状态下的两个价值函数 $V(s)$ 和 $V'(s)$。我们如下证明贝尔曼更新 $B$ 在最大范数下是一个以 $\gamma \in (0,1)$ 为收缩因子的收缩映射：

$$
\begin{aligned}
|BV(s) - BV'(s)|
&= |\max_a Q_V(s,a) - \max_a Q_{V'}(s,a)| \\
&\le \max_a |Q_V(s,a) - Q_{V'}(s,a)| \\
&= \max_a \left|
\gamma \sum_{s'} T(s,a,s')(V(s') - V'(s'))
\right| \\
&= \gamma \max_a \left|
\sum_{s'} T(s,a,s')(V(s') - V'(s'))
\right| \\
&\le \gamma \max_a
\sum_{s'} T(s,a,s') |V(s') - V'(s')| \\
&\le \gamma \max_{s'} |V(s') - V'(s')| \\
&= \gamma \|V - V'\|_\infty
\end{aligned}
$$


其中，第一个不等式来自前面介绍的一般不等式；第二个不等式来自对 $V$ 和 $V'$ 之间差值取最大值；倒数第二步利用了概率在任意动作 $a$ 下总和为 1 的事实；最后一步使用了向量 $x = (x_1, \dots, x_n)$ 的最大范数定义：

$$
\|x\|_\infty = \max(|x_1|, \dots, |x_n|).
$$

由于我们刚刚证明了通过贝尔曼更新进行的值迭代是一个以 $\gamma$ 为收缩因子的收缩映射，因此我们知道值迭代一定收敛，并且收敛发生在达到满足

$$
V^* = B U^*
$$

的固定点时。

---

现在我们通过回顾之前的赛车 MDP 示例来实际观察几轮值迭代更新，并引入折扣因子 $\gamma = 0.5$。

### 赛车 MDP

我们首先通过初始化所有

$$
V_0(s) = 0
$$

来开始值迭代：

|   | cool | warm | overheated |
|---|------|------|------------|
| V₀ | 0 | 0 | 0 |

在第一轮更新中，我们计算 $\forall s \in S,\; V_1(s)$ 如下：

$$
\begin{aligned}
V_1(cool)
&= \max_a Q_0(cool,a) \\
&= \max(1, 2) \\
&= 2
\end{aligned}
$$

$$
\begin{aligned}
V_1(warm)
&= \max_a Q_0(warm,a) \\
&= \max(1, -10) \\
&= 1
\end{aligned}
$$



$$
\begin{aligned}
V_1(overheated)
&= 0
\end{aligned}
$$

|   | cool | warm | overheated |
|---|------|------|------------|
| U₀ | 0 | 0 | 0 |
| U₁ | 2 | 1 | 0 |

同样地，我们利用 $U_1(s)$ 计算 $V_2(s)$：

$$
\begin{aligned}
V_2(cool)
&= \max(1 \cdot (1 + 0.5 \cdot 2),
0.5 \cdot (2 + 0.5 \cdot 2) + 0.5 \cdot (2 + 0.5 \cdot 1)) \\
&= \max(2, 2.75) \\
&= 2.75
\end{aligned}
$$

$$
\begin{aligned}
V_2(warm)
&= \max(0.5 \cdot (1 + 0.5 \cdot 2) + 0.5 \cdot (1 + 0.5 \cdot 1),
1 \cdot (-10 + 0.5 \cdot 0)) \\
&= \max(1.75, -10) \\
&= 1.75
\end{aligned}
$$


$$
V_2(overheated) = \max\{\;\} = 0
$$




|   | cool | warm | overheated |
|---|------|------|------------|
| V₀ | 0 | 0 | 0 |
| V₁ | 2 | 1 | 0 |
| V₂ | 2.75 | 1.75 | 0 |

值得注意的是，对于任何终止状态，$V^*(s)$ 必须为 0，因为从终止状态出发无法再采取任何行动来获得奖励。

### 4.3.1 策略提取（Policy Extraction）

回想一下，我们解决 MDP 的最终目标是确定一个最优策略。一旦求出了所有状态的最优值，就可以使用一种称为 **策略提取**（policy extraction）的方法来完成。  

策略提取的直觉非常简单：如果你处于状态 \(s\)，你应该选择能够产生最大期望效用的动作 \(a\)。不出意外，\(a\) 就是能够让我们进入具有最大 Q 值的 Q 状态的动作，从而可以给最优策略一个形式化定义：

$$
\forall s \in S, \quad
\pi^*(s) = \arg\max_a Q^*(s,a)
= \arg\max_a \sum_{s'} T(s,a,s') \big[ R(s,a,s') + \gamma V^*(s') \big]
$$

出于性能考虑，有一点很重要：如果我们在策略提取时已经存储了状态的最优 Q 值，那么确定一个状态的最优动作只需要一个简单的 \(\arg\max\) 操作即可。  

如果只存储每个状态的最优值 \(V^*(s)\)，那么在应用 \(\arg\max\) 之前必须先用贝尔曼方程重新计算所有必要的 Q 值，这相当于执行了一次深度为 1 的 expectimax 搜索。

---

### 4.3.2 Q 值迭代（Q-Value Iteration）

在使用 **值迭代**（value iteration）求最优策略时，我们首先找到所有状态的最优值，然后通过策略提取得到最优策略。然而，你可能已经注意到，我们还可以处理另一种包含最优策略信息的值：**Q 值**。

**Q 值迭代**（Q-value iteration）是一种动态规划算法，用于计算时间受限的 Q 值，其更新公式如下：

$$
Q_{k+1}(s,a) \gets \sum_{s'} T(s,a,s') \Big[ R(s,a,s') + \gamma \max_{a'} Q_k(s',a') \Big]
$$

注意，这条更新规则和值迭代的更新规则几乎相同。唯一的区别在于 \(\max\) 运算的位置：  

- 当我们处于一个状态 \(s\) 时，选择动作发生在转移之前  
- 当我们处于一个 Q 状态时，选择动作发生在转移之后  

一旦我们求得每个状态和动作的最优 Q 值，就可以通过简单地选择具有最大 Q 值的动作来得到该状态的最优策略。

