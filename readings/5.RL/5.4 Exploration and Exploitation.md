### 5.4 探索与利用（Exploration and Exploitation）

到目前为止，我们已经介绍了多种让智能体学习最优策略的方法，并强调了“充分探索（sufficient exploration）”的重要性，但并没有详细说明“充分”到底意味着什么。在接下来的内容中，我们将讨论两种在探索与利用之间分配时间的方法：**ε-贪婪策略（ε-greedy policies）** 和 **探索函数（exploration functions）**。

---

#### 5.4.1 ε-贪婪策略（ε-Greedy Policies）

遵循 **ε-贪婪策略** 的智能体定义一个概率 \(0 \le \epsilon \le 1\)：  

- 以概率 \(\epsilon\) 随机选择动作并进行探索（exploration）  
- 以概率 \(1-\epsilon\) 按照当前已建立的策略进行利用（exploitation）

这种策略实现起来非常简单，但在实际操作中仍有难点：  

- 如果选择较大的 \(\epsilon\)，即使智能体已经学到了最优策略，它仍会大部分时间随机行动。  
- 如果选择较小的 \(\epsilon\)，智能体很少探索，导致 Q-learning（或其他学习算法）学习最优策略的速度非常慢。  

因此，\(\epsilon\) 通常需要手动调节，并随着时间逐渐减小，以获得良好的学习效果。

---

#### 5.4.2 探索函数（Exploration Functions）

通过**探索函数**可以避免手动调节 \(\epsilon\) 的问题。探索函数通过对 Q 值迭代更新进行修改，给访问次数较少的状态以一定偏好。修改后的更新公式为：

\[
Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha \cdot \Big[ R(s,a,s') + \gamma \max_{a'} f(s',a') \Big]
\]

其中 \(f\) 表示探索函数。探索函数的设计有一定灵活性，但常用的一种形式是：

\[
f(s,a) = Q(s,a) + \frac{k}{N(s,a)}
\]

- \(k\) 是预先设定的常数  
- \(N(s,a)\) 表示 Q-状态 \((s,a)\) 被访问的次数  

智能体在状态 \(s\) 中总是选择具有最高 \(f(s,a)\) 的动作，因此无需在探索与利用之间进行概率决策。探索自然通过探索函数实现：  
- 当某些动作很少被选择时，额外的“奖励” \(\frac{k}{N(s,a)}\) 可能让它们的 \(f(s,a)\) 高于 Q 值较高的动作，从而被选择。  
- 随着状态被访问次数增加，这个奖励逐渐趋近于 0，\(f(s,a)\) 回归到 \(Q(s,a)\)，智能体逐渐只进行利用（exploitation）。
