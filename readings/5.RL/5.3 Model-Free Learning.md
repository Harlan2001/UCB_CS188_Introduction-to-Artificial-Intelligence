## 5.3 无模型学习（Model-Free Learning）

接下来讨论无模型学习！无模型学习算法有几种，这里我们将介绍三种：直接评估（Direct Evaluation）、时序差分学习（Temporal Difference Learning）和 Q 学习（Q-Learning）。  

- 直接评估和时序差分学习属于**被动强化学习**（Passive Reinforcement Learning）算法。在被动强化学习中，智能体被赋予一个固定策略，并在经历回合时学习在该策略下的状态价值，这与已知 \(T\) 和 \(R\) 时对 MDP 进行策略评估的做法完全相同。  
- Q 学习属于另一类无模型学习算法，即**主动强化学习**（Active Reinforcement Learning）。在主动强化学习中，学习智能体可以利用所收到的反馈来迭代更新策略，并在充分探索后最终确定最优策略。

---

### 5.3.1 直接评估（Direct Evaluation）

第一种被动强化学习技术是直接评估，其方法正如名字所示——非常简单和直接。  
直接评估所做的就是固定某个策略 \(\pi\)，让智能体在多个回合中按照该策略 \(\pi\) 执行动作。在经历这些回合时，智能体收集样本，并维护每个状态的总获得效用以及访问次数。在任意时刻，可以通过将某状态 \(s\) 的总效用除以该状态的访问次数来计算该状态的估计价值：

\[
V_\pi(s) = \frac{\text{Total utility obtained from } s}{\text{Number of times } s \text{ was visited}}
\]

我们在之前的示例上运行直接评估，记住折扣因子 \(\gamma = 1\)。

#### 示例

在第一回合中：

- 从状态 \(D\) 到终止状态，总奖励为 \(10\)  
- 从状态 \(C\) 到终止状态，总奖励为 \((-1) + 10 = 9\)  
- 从状态 \(B\) 到终止状态，总奖励为 \((-1) + (-1) + 10 = 8\)

完成所有回合后，每个状态的总奖励与访问次数，以及由此计算出的估计价值如下表：

| s | Total Reward | Times Visited | \(V_\pi(s)\) |
|---|--------------|---------------|---------------|
| A | -10          | 1             | -10           |
| B | 16           | 2             | 8             |
| C | 16           | 4             | 4             |
| D | 30           | 3             | 10            |
| E | -4           | 2             | -2            |

尽管直接评估最终会学习到每个状态的价值，但收敛通常较慢，因为它没有充分利用状态之间的转移信息。

#### 注释示例

在我们的示例中，计算得到：

\[
V_\pi(E) = -2, \quad V_\pi(B) = 8
\]

然而根据反馈，这两个状态的唯一后继状态都是 \(C\)，且转移到 \(C\) 时的奖励均为 \(-1\)。根据贝尔曼方程，这意味着 \(B\) 和 \(E\) 在策略 \(\pi\) 下应具有相同的价值。  

之所以出现偏差，是因为在 4 次处于状态 \(C\) 的情况下，有 3 次转移到 \(D\) 并获得奖励 10，有 1 次转移到 \(A\) 并获得奖励 -10。恰巧那一次获得 -10 奖励是从状态 \(E\) 开始，而不是 \(B\)，这严重影响了 \(E\) 的估计价值。  

随着回合数的增加，\(B\) 和 \(E\) 的价值会收敛到真实值，但这种情况会导致过程比预期慢。  
这个问题可以通过使用第二种被动强化学习算法——**时序差分学习（Temporal Difference Learning）**来缓解。

### 5.3.2 时序差分学习（Temporal Difference Learning）

时序差分学习（TD 学习）利用了“从每一次经历中学习”的思想，而不是像直接评估那样仅仅记录总奖励和状态访问次数，在最后才进行学习。  

在策略评估中，我们使用固定策略和贝尔曼方程生成的方程组来确定该策略下的状态价值（或者像值迭代一样使用迭代更新）：

\[
V_\pi(s) = \sum_{s'} T(s, \pi(s), s') \left[ R(s, \pi(s), s') + \gamma V_\pi(s') \right]
\]

每个方程将某个状态的价值与其所有后继状态的折扣价值加上转移奖励的加权平均联系起来。TD 学习试图在没有权重信息的情况下计算这个加权平均值，并通过**指数移动平均**巧妙地实现。  

#### TD 学习步骤

1. 初始化：对所有状态 \(s\)：
\[
\forall s, \quad V_\pi(s) = 0
\]

2. 在每个时间步，智能体从状态 \(s\) 采取动作 \(\pi(s)\)，转移到状态 \(s'\)，并获得奖励 \(R(s, \pi(s), s')\)。  

3. 可以通过下面公式获得一个样本值：

\[
\text{sample} = R(s, \pi(s), s') + \gamma V_\pi(s')
\]

这个样本就是状态 \(s\) 的新的价值估计。  

4. 使用指数移动平均将这个样本整合进现有模型中，更新规则为：

\[
V_\pi(s) \leftarrow (1 - \alpha) V_\pi(s) + \alpha \cdot \text{sample}
\]

其中 \(\alpha\) 是学习率参数，满足 \(0 \le \alpha \le 1\)，用于控制对现有估计 \(V_\pi(s)\) 的权重 \(1-\alpha\) 以及对新样本的权重 \(\alpha\)。  

- 通常可以从 \(\alpha = 1\) 开始，这样初始 \(V_\pi(s)\) 就等于第一次样本的值，然后逐渐减小 \(\alpha\)，使得后续样本对 \(V_\pi(s)\) 的影响逐渐减小。

---

#### 更新规则递归展开

为分析方便，定义：

- \(V_\pi^k(s)\)：第 \(k\) 次更新后的状态 \(s\) 的估计值  
- \(\text{sample}_k\)：第 \(k\) 次样本值  

则更新规则可写为递归形式：

\[
V_\pi^k(s) \leftarrow (1-\alpha) V_\pi^{k-1}(s) + \alpha \cdot \text{sample}_k
\]

展开递归：

\[
V_\pi^k(s) \leftarrow \alpha \left[ (1-\alpha)^{k-1} \text{sample}_1 + \dots + (1-\alpha)\text{sample}_{k-1} + \text{sample}_k \right]
\]

由于 \(0 \le (1-\alpha) \le 1\)，随着指数幂增大，旧样本的权重会指数衰减接近 0。  

这正是 TD 学习的优点：  

1. **每一步都学习**：使用实时获取的状态转移信息，而不是等到回合结束才计算。  
2. **指数衰减旧样本权重**：旧样本可能不准确（因为基于较旧的模型），权重逐步减小。  
3. **更快收敛**：比直接评估需要的回合数更少，也能更快接近真实状态价值。

### 5.3.3 Q-学习（Q-Learning）

直接评估和 TD 学习最终都会学到它们所遵循策略下的状态真实价值。然而，它们都有一个固有的主要问题——我们希望找到智能体的**最优策略**，这需要知道状态的 Q 值。要从已有的状态价值计算 Q 值，我们需要根据贝尔曼方程使用转移函数和奖励函数：

\[
Q^*(s,a) = \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^*(s') \right]
\]

因此，TD 学习或直接评估通常需要与某种**基于模型的学习**结合，先获取 \(T\) 和 \(R\) 的估计值，才能有效更新智能体的策略。  

然而，一种革命性的新方法——**Q-learning**——提出直接学习状态的 Q 值，从而绕过对状态价值、转移函数和奖励函数的任何依赖。因此，Q-learning 完全是**无模型的（model-free）**。  

#### Q-learning 更新规则（Q-value Iteration）

Q-learning 使用以下更新规则进行 Q 值迭代：

\[
Q_{k+1}(s,a) \leftarrow \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma \max_{a'} Q_k(s',a') \right]
\]

注意，这个更新规则与值迭代的更新规则只有轻微区别：  
- 唯一的区别在于 **max 操作的位置**。  
- 当我们处于状态 \(s\) 时，我们先选择动作再转移；而在 Q 状态中，则是先转移再选择新动作。  

---

#### Q-learning 样本与指数移动平均

使用这个更新规则，Q-learning 本质上和 TD 学习类似，通过获取 Q 值样本：

\[
\text{sample} = R(s,a,s') + \gamma \max_{a'} Q(s',a')
\]

并将样本整合进指数移动平均：

\[
Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha \cdot \text{sample}
\]

只要我们在探索中花费足够时间，并以适当速度逐渐减小学习率 \(\alpha\)，Q-learning 就能学习到每个 Q 状态的最优 Q 值。  

---

#### Q-learning 的革命性

- TD 学习和直接评估需要遵循策略才能学习状态价值，再通过其他方法确定策略最优性。  
- Q-learning 可以**直接学习最优策略**，即使采取的是次优或随机动作。  

这种方法称为**离策略学习（off-policy learning）**，与直接评估和 TD 学习的**同策略学习（on-policy learning）**形成对比。

### 5.3.4 近似 Q-学习（Approximate Q-Learning）

Q-learning 是一种非常出色的学习技术，它仍然是强化学习领域发展的核心。然而，它仍有改进空间。现有的 Q-learning 以表格形式存储所有状态的 Q 值，这在大多数强化学习应用中效率不高，因为这些应用可能有数千甚至数百万个状态。这意味着我们在训练时无法访问所有状态，即使能够访问，也可能由于内存限制无法存储所有 Q 值。

![](../../image/5.RL/qlearn-pac-1.png)

![](../../image/5.RL/qlearn-pac-2.png)

![](../../image/5.RL/qlearn-pac-3.png)

---

例如，假设 Pacman 通过普通 Q-learning 学习到图 1 是不利的，那么它仍然无法知道图 2 或图 3 也同样不利。**近似 Q-learning** 试图解决这个问题：通过学习一些通用情况，并将经验推广到其他相似情况。实现这一目标的关键是**基于特征的状态表示**，即将每个状态表示为一个称为特征向量（feature vector）的向量。例如，Pacman 的特征向量可能包含：

- 到最近幽灵的距离  
- 到最近食物颗粒的距离  
- 幽灵数量  
- Pacman 是否被困（0 或 1）

---

通过特征向量，我们可以将状态和 Q-状态的值视为**线性价值函数**：

\[
V(s) = w_1 \cdot f_1(s) + w_2 \cdot f_2(s) + \dots + w_n \cdot f_n(s) = \vec{w} \cdot \vec{f}(s)
\]

\[
Q(s,a) = w_1 \cdot f_1(s,a) + w_2 \cdot f_2(s,a) + \dots + w_n \cdot f_n(s,a) = \vec{w} \cdot \vec{f}(s,a)
\]

其中：

\[
\vec{f}(s) = 
\begin{bmatrix}
f_1(s) \\ f_2(s) \\ \vdots \\ f_n(s)
\end{bmatrix}, 
\quad
\vec{f}(s,a) = 
\begin{bmatrix}
f_1(s,a) \\ f_2(s,a) \\ \vdots \\ f_n(s,a)
\end{bmatrix}, 
\quad
\vec{w} = 
\begin{bmatrix}
w_1 \\ w_2 \\ \vdots \\ w_n
\end{bmatrix}
\]

分别表示状态 \(s\) 和 Q-状态 \((s,a)\) 的特征向量，以及权重向量。

---

定义误差（difference）为：

\[
\text{difference} = \left[ R(s,a,s') + \gamma \max_{a'} Q(s',a') \right] - Q(s,a)
\]

近似 Q-learning 的更新规则为：

\[
w_i \leftarrow w_i + \alpha \cdot \text{difference} \cdot f_i(s,a)
\]

与普通 Q-learning 不同的是，我们不需要为每个状态存储 Q 值，而只需存储一个权重向量，并可以按需计算 Q 值。这不仅实现了 **更通用的 Q-learning**，还显著节省了内存。

---

作为对 Q-learning 的补充，我们也可以将精确 Q-learning 的更新规则用 difference 表示为：

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \cdot \text{difference}
\]

这个表示方式提供了另一种解释：它计算**采样估计值与当前 Q(s,a) 模型的差异**，并沿着估计值的方向更新模型，更新幅度与差异大小成比例。

