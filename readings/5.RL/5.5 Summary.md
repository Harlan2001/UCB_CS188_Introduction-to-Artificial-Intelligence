### 5.5 总结（Summary）

需要特别注意的是，强化学习的基础是一个 **马尔可夫决策过程（MDP）**，强化学习的目标就是通过推导出最优策略来求解这个 MDP。  

强化学习与像 **值迭代（value iteration）** 或 **策略迭代（policy iteration）** 等方法的区别在于：强化学习中，智能体并不知道底层 MDP 的转移函数 \(T\) 和奖励函数 \(R\)。因此，智能体必须通过 **在线试错（online trial-and-error）** 来学习最优策略，而不是单纯地进行离线计算（offline computation）。

实现这一目标的方法主要有以下几类：

---

#### 1. 基于模型的学习（Model-based learning）

通过计算来估计转移函数 \(T\) 和奖励函数 \(R\) 的值，然后利用这些估计值结合 MDP 求解方法（如值迭代或策略迭代）来推导最优策略。

---

#### 2. 无模型学习（Model-free learning）

避免估计 \(T\) 和 \(R\)，而是直接使用其他方法估计状态值或 Q 值。

---

#### 3. 直接估计（Direct Evaluation）

遵循某一策略 \(\pi\)，记录每个状态获得的总奖励以及访问次数。  
- 如果样本量足够大，这种方法会收敛到该策略下每个状态的真实值 \(V^\pi(s)\)  
- 但是收敛速度慢，而且会浪费关于状态转移的信息  

---

#### 4. 时序差分学习（Temporal Difference Learning, TD Learning）

遵循策略 \(\pi\)，使用 **指数移动平均（exponential moving average）** 将采样值整合到已有的状态值估计中，直到收敛到该策略下的真实状态值。  

- TD 学习和直接估计都是 **在策略学习（on-policy learning）** 的例子，即先学习特定策略下的状态值，再判断该策略是否次优并进行更新。

---

#### 5. Q 学习（Q-Learning）

通过试错和 **Q 值迭代（Q-value iteration）** 直接学习最优策略。  
- 这是 **离策略学习（off-policy learning）** 的例子，即即使采取次优动作，也能最终学习到最优策略。

---

#### 6. 近似 Q 学习（Approximate Q-Learning）

与 Q 学习类似，但使用 **基于特征的状态表示（feature-based representation）** 来泛化学习，从而在大规模状态空间下更高效。

---

#### 7. 性能评估：后悔值（Regret）

为了量化不同强化学习算法的性能，我们引入 **后悔值（regret）** 的概念：  

\[
\text{Regret} = \text{如果从一开始就采取最优策略所获得的总奖励} - \text{运行学习算法实际获得的总奖励}
\]

- 后悔值越小，算法的性能越接近最优策略。
