## 5.1 强化学习

在上一篇笔记中，我们讨论了马尔可夫决策过程（Markov Decision Processes, MDP），并使用值迭代（Value Iteration）和策略迭代（Policy Iteration）等技术来求解状态的最优值并提取最优策略。求解 MDP 是一种**离线规划（offline planning）**的例子，其中智能体对环境的转移函数和奖励函数有完整的知识，也就是说，它们拥有所有信息，可以在不实际执行任何动作的情况下，预先计算出在 MDP 所描述的世界中最优的行动。

在本篇笔记中，我们将讨论**在线规划（online planning）**，此时智能体对环境中的奖励和转移没有任何先验知识（环境仍然用 MDP 表示）。在在线规划中，智能体必须进行**探索（exploration）**，即实际执行动作，并根据到达的后继状态及获得的奖励获得反馈。智能体利用这些反馈通过一种称为**强化学习（reinforcement learning）**的过程来估计最优策略，然后使用该估计策略进行**利用（exploitation）**或奖励最大化。

![](../../image/5.RL/feedback-loop.png)

### 反馈循环（Feedback Loop）

首先介绍一些基本术语。在在线规划的每个时间步中，智能体从状态 `s` 出发，执行动作 `a`，最终到达后继状态 `s'`，并获得奖励 `r`。每个四元组 `(s, a, s', r)` 称为一个**样本（sample）**。通常，智能体会连续执行动作并收集样本，直到到达一个终止状态（terminal state）。这种连续收集的样本集合称为**一个回合（episode）**。智能体在探索过程中通常会经历许多回合，以收集足够的数据用于学习。

强化学习主要分为两类：**基于模型的学习（model-based learning）**和**无模型学习（model-free learning）**。  
- **基于模型的学习**：智能体利用探索中获得的样本去估计环境的转移函数和奖励函数，然后使用这些估计值按照正常的 MDP 方法（如值迭代或策略迭代）求解最优策略。  
- **无模型学习**：智能体直接估计状态值（state values）或 Q 值（Q-values），而无需使用任何内存去构建 MDP 的奖励和转移模型。
